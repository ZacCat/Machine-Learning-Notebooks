@Article{wan-lei-2018-k-means-a-revisit,
  title = {k-means: A revisit},
  journal = {Neurocomputing},
  volume = {291},
  pages = {195 - 206},
  year = {2018},
  issn = {0925-2312},
  doi = {https://doi.org/10.1016/j.neucom.2018.02.072},
  url = {http://www.sciencedirect.com/science/article/pii/S092523121830239X},
  author = {Wan-Lei Zhao and Cheng-Hao Deng and Chong-Wah Ngo},
  keywords = {Clustering, -means, Incremental optimization},

  abstract={Due to its simplicity and versatility, k-means remains popular since it was proposed three decades ago. The performance of k-means has been enhanced from different perspectives over the years. Unfortunately, a good trade-off between quality and efficiency is hardly reached. In this paper, a novel k-means variant is presented. Different from most of k-means variants, the clustering procedure is driven by an explicit objective function, which is feasible for the whole l2-space. The classic egg-chicken loop in k-means has been simplified to a pure stochastic optimization procedure. The procedure of k-means becomes simpler and converges to a considerably better local optima. The effectiveness of this new variant has been studied extensively in different contexts, such as document clustering, nearest neighbor search and image clustering. Superior performance is observed across different scenarios.}
}

@INPROCEEDINGS{sculley-2010-web-scale-k-means-clustering,
 author = {Sculley, D.},
 title = {Web-scale K-means Clustering},
 booktitle = {Proceedings of the 19th International Conference on World Wide Web},
 series = {WWW '10},
 year = {2010},
 isbn = {978-1-60558-799-8},
 location = {Raleigh, North Carolina, USA},
 pages = {1177--1178},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1772690.1772862},
 doi = {10.1145/1772690.1772862},
 acmid = {1772862},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {scalability, sparse solutions, unsupervised clustering},

 abstract={We present two modifications to the popular k-means clustering algorithm to address the extreme requirements for latency, scalability, and sparsity encountered in user-facing web applications. First, we propose the use of mini-batch optimization for k-means clustering. This reduces computation cost by orders of magnitude compared to the classic batch algorithm while yielding significantly better solutions than online stochastic gradient descent. Second, we achieve sparsity with projected gradient descent, and give a fast Îµ-accurate projection onto the L1-ball. Source code is freely available: http://code.google.com/p/sofia-ml}
} 

@Article{kanungo-2002-an-efficient-k-means-clustering-algorithm-analysis-and-implementation, 

  author={ Kanungo, T. and Mount, D. M. and Netanyahu, N. S. and Piatko, C. D. and Silverman, R. and Wu, A. Y. }, 
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={An efficient k-means clustering algorithm: analysis and implementation}, 
  year={2002}, 
  volume={24}, 
  number={7}, 
  pages={881-892}, 
  
  abstract={In k-means clustering, we are given a set of n data points in d-dimensional space Rd and an integer k and the problem is to determine a set of k points in Rd, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd's (1982) algorithm. We present a simple and efficient implementation of Lloyd's k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation}, 
  
  keywords={covariance matrices;filtering theory;pattern clustering;Lloyd algorithm;color quantization;data compression;data structure;data-sensitive analysis;filtering algorithm;image segmentation;k-means clustering algorithm;kd-tree;mean squared distance;Algorithm design and analysis;Clustering algorithms;Data analysis;Data compression;Data mining;Data structures;Filtering algorithms;Iterative algorithms;Machine learning algorithms;Pattern recognition}, 
  
  doi={10.1109/TPAMI.2002.1017616}, 
  ISSN={0162-8828}, 
  month={Jul}
}

@INPROCEEDINGS{wagstaff-2001-constrained-k-means-clustering-with-background-knowledge,
  title={Constrained k-means clustering with background knowledge},
  author={Wagstaff, Kiri and Cardie, Claire and Rogers, Seth and Schr{\"o}dl, Stefan and others},
  booktitle={ICML},
  volume={1},
  pages={577--584},
  year={2001},

  abstract={Clustering is traditionally viewed as an unsupervised method for data analysis. However, in some cases information about the problem domain is available in addition to the data instances themselves. In this paper, we demonstrate how the popular k-means clustering algorithm can be profitably modified to make use of this information. In experiments with artificial constraints on six data sets, we observe improvements in clustering accuracy. We also apply this method to the real-world problem of automatically detecting road lanes from GPS data and observe dramatic increases in performance.}
}

@Article{cui-2014-optimized-big-data-k-means-clustering-using-mapreduce,
  author={Cui, Xiaoli and Zhu, Pingfei and Yang, Xin and Li, Keqiu and Ji, Changqing},
  title={Optimized big data K-means clustering using MapReduce},
  journal={The Journal of Supercomputing},
  year={2014},
  month={Dec},
  day={01},
  volume={70},
  number={3},
  pages={1249--1259},

  abstract={Clustering analysis is one of the most commonly used data processing algorithms. Over half a century, K-means remains the most popular clustering algorithm because of its simplicity. Recently, as data volume continues to rise, some researchers turn to MapReduce to get high performance. However, MapReduce is unsuitable for iterated algorithms owing to repeated times of restarting jobs, big data reading and shuffling. In this paper, we address the problems of processing large-scale data using K-means clustering algorithm and propose a novel processing model in MapReduce to eliminate the iteration dependence and obtain high performance. We analyze and implement our idea. Extensive experiments on our cluster demonstrate that our proposed methods are efficient, robust and scalable.},

  issn={1573-0484},
  doi={10.1007/s11227-014-1225-7},
  url={https://doi.org/10.1007/s11227-014-1225-7}
}

@article{alsabti-1997-an-efficient-k-means-clustering-algorithm,
  title={An efficient k-means clustering algorithm},
  author={Alsabti, Khaled and Ranka, Sanjay and Singh, Vineet},
  year={1997},
  journal={Electrical Engineering and Computer Science},
  volume={43},

  abstract={In this paper, we present a novel algorithm for performing k-means clustering. It organizes all the patterns in a k-d tree structure such that one can find all the patterns which are closest to a given prototype efficiently. The main intuition behind our approach is as follows. All the prototypes are potential candidates for the closest prototype at the root level. However, for the children of the root node, we may be able to prune the candidate set by using simple geometrical constraints. This approach can be applied recursively until the size of the candidate set is one for each node. Our experimental results demonstrate that our scheme can improve the computational speed of the direct k-means algorithm by an order to two orders of magnitude in the total number of distance calculations and the overall time of computation.}
}
