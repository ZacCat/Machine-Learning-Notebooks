@Article{bentley-1975-multidimensional-binary-search-trees-used-for-associative-searching,
 author = {Bentley, Jon Louis},
 title = {Multidimensional Binary Search Trees Used for Associative Searching},
 journal = {Commun. ACM},
 volume = {18},
 number = {9},
 month = {sep},
 year = {1975},
 issn = {0001-0782},
 pages = {509--517},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/361002.361007},
 doi = {10.1145/361002.361007},
 acmid = {361007},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {associative retrieval, attribute, binary search trees, binary tree insertion, information retrieval system, intersection queries, key, nearest neighbor queries, partial match queries},

 abstract={This paper develops the multidimensional binary search tree (or k-d tree, where k is the dimensionality of the search space) as a data structure for storage of information to be retrieved by associative searches. The k-d tree is defined and examples are given. It is shown to be quite efficient in its storage requirements. A significant advantage of this structure is that a single data structure can handle many types of queries very efficiently. Various utility algorithms are developed; their proven average running times in an n record file are: insertion, O(log n); deletion of the root, O(n(k-1)/k); deletion of a random node, O(log n); and optimization (guarantees logarithmic performance of searches), O(n log n). Search algorithms are given for partial match queries with t keys specified [proven maximum running time of O(n(k-t)/k)] and for nearest neighbor queries [empirically observed average running time of O(log n).] These performances far surpass the best currently known algorithms for these tasks. An algorithm is presented to handle any general intersection query. The main focus of this paper is theoretical. It is felt, however, that k-d trees could be quite useful in many applications, and examples of potential uses are given.}
} 

@Article{cover-1967-nearest-neighbor-pattern-classification, 
  author={Cover, T. and Hart, P.}, 
  journal={IEEE Transactions on Information Theory}, 
  title={Nearest neighbor pattern classification}, 
  year={1967}, 
  volume={13}, 
  number={1}, 
  pages={21-27}, 

  abstract={The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of   previously classified points. This rule is independent of the underlying joint distribution on the sample points and their   classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR^{ast}  --the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large   sample analysis, we will show in theM-category case thatR^{ast} leq R leq R^{ast}(2 --MR^{ast}/(M-1)), where these bounds are the   tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the   nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the   classification information in an infinite sample set is contained in the nearest neighbor.},
  
  keywords={Pattern classification}, 
  doi={10.1109/TIT.1967.1053964}, 
  ISSN={0018-9448}, 
  month={January},
}

@TECHREPORT{omohundro-1989-five-balltree-construction-algorithms,
  author = {Omohundro, Stephen},
  title = {Five Balltree Construction Algorithms},
  institution = {International Computer Science Institute Berkeley},
  year = {1989},
  number={TR-89-063},

  abstract={Balltrees are simple geometric data structures with a wide range of practical applications to geometric learning tasks. In this report we compare 5 different algorithms for constructing balltrees from data. We study the trade-off between construction time and the quality of the constructed tree. Two of the algorithms are on-line, two construct the structures from the data set in a top down fashion, and one uses a bottom up approach.}
}

@Article{garcia-2012-prototype-selection-for-nearest-neighbor-classification:-taxonomy-and-empirical-study, 
  author={Garcia, S. and Derrac, J. and Cano, J. and Herrera, F.}, 
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Prototype Selection for Nearest Neighbor Classification: Taxonomy and Empirical Study}, 
  year={2012}, 
  volume={34}, 
  number={3}, 
  pages={417-435},

  abstract={The nearest neighbor classifier is one of the most used and well-known techniques for performing recognition tasks. It has also demonstrated itself to be one of the most useful algorithms in data mining in spite of its simplicity. However, the nearest neighbor classifier suffers from several drawbacks such as high storage requirements, low efficiency in classification response, and low noise tolerance. These weaknesses have been the subject of study for many researchers and many solutions have been proposed. Among them, one of the most promising solutions consists of reducing the data used for establishing a classification rule (training data) by means of selecting relevant prototypes. Many prototype selection methods exist in the literature and the research in this area is still advancing. Different properties could be observed in the definition of them, but no formal categorization has been established yet. This paper provides a survey of the prototype selection methods proposed in the literature from a theoretical and empirical point of view. Considering a theoretical point of view, we propose a taxonomy based on the main characteristics presented in prototype selection and we analyze their advantages and drawbacks. Empirically, we conduct an experimental study involving different sizes of data sets for measuring their performance in terms of accuracy, reduction capabilities, and runtime. The results obtained by all the methods studied have been verified by nonparametric statistical tests. Several remarks, guidelines, and recommendations are made for the use of prototype selection for nearest neighbor classification.}, 
  keywords={data mining;pattern classification;data mining;machine learning;nearest neighbor classification;prototype selection methods;Accuracy;Classification algorithms;Noise;Noise measurement;Prototypes;Taxonomy;Training;Prototype selection;classification.;condensation;edition;nearest neighbor;taxonomy}, 
  doi={10.1109/TPAMI.2011.142}, 
  ISSN={0162-8828}, 
  month={March}
}

@Article{hart-1968-the-condensed-nearest-neighbor-rule, 
  author={P. Hart}, 
  journal={IEEE Transactions on Information Theory}, 
  title={The condensed nearest neighbor rule (Corresp.)}, 
  year={1968}, 
  volume={14}, 
  number={3}, 
  pages={515-516}, 
  abstract={Not Available}, 
  keywords={Pattern classification;Cellular neural networks;Error analysis;Information theory;Nearest neighbor searches;Neural   networks;Performance evaluation;Piecewise linear techniques;Sampling methods;Testing;Voting}, 
  doi={10.1109/TIT.1968.1054155}, 
  ISSN={0018-9448}, 
  month={May}
}

@INPROCEEDINGS{shaw-2009-structure-preserving-embedding,
 author = {Shaw, Blake and Jebara, Tony},
 title = {Structure Preserving Embedding},
 booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
 series = {ICML '09},
 year = {2009},
 isbn = {978-1-60558-516-1},
 location = {Montreal, Quebec, Canada},
 pages = {937--944},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1553374.1553494},
 doi = {10.1145/1553374.1553494},
 acmid = {1553494},
 publisher = {ACM},
 address = {New York, NY, USA},
 abstract={Structure Preserving Embedding (SPE) is an algorithm for embedding graphs in Euclidean space such that the embedding is low-dimensional and preserves the global topological properties of the input graph. Topology is preserved if a connectivity algorithm, such as k-nearest neighbors, can easily recover the edges of the input graph from only the coordinates of the nodes after embedding. SPE is formulated as a semidefinite program that learns a low-rank kernel matrix constrained by a set of linear inequalities which captures the connectivity structure of the input graph. Traditional graph embedding algorithms do not preserve structure according to our definition, and thus the resulting visualizations can be misleading or less informative. SPE provides significant improvements in terms of visualization and lossless compression of graphs, outperforming popular methods such as spectral embedding and Laplacian eigen-maps. We find that many classical graphs and networks can be properly embedded using only a few dimensions. Furthermore, introducing structure preserving constraints into dimensionality reduction algorithms produces more accurate representations of high-dimensional data.}
} 

@Article{hall-2008-choice-of-neighbor-order-in-nearest-neighbor-classification,
  title = {Choice of neighbor order in nearest-neighbor classification},
  author = {Hall, Peter and Park, Byeong U. and Samworth, Richard J.},
  doi = {10.1214/07-AOS537},
  fjournal = {The Annals of Statistics},
  journal = {Ann. Statist.},
  month = {10},
  number = {5},
  pages = {2135--2152},
  publisher = {The Institute of Mathematical Statistics},
  url = {https://doi.org/10.1214/07-AOS537},
  volume = {36},
  year = {2008},
  abstract={The kth-nearest neighbor rule is arguably the simplest and most intuitively appealing nonparametric classification procedure. However, application of this method is inhibited by lack of knowledge about its properties, in particular, about the manner in which it is influenced by the value of k; and by the absence of techniques for empirical choice of k. In the present paper we detail the way in which the value of k determines the misclassification error. We consider two models, Poisson and Binomial, for the training samples. Under the first model, data are recorded in a Poisson stream and are “assigned” to one or other of the two populations in accordance with the prior probabilities. In particular, the total number of data in both training samples is a Poisson-distributed random variable. Under the Binomial model, however, the total number of data in the training samples is fixed, although again each data value is assigned in a random way. Although the values of risk and regret associated with the Poisson and Binomial models are different, they are asymptotically equivalent to first order, and also to the risks associated with kernel-based classifiers that are tailored to the case of two derivatives. These properties motivate new methods for choosing the value of k.}
}

@INPROCEEDINGS{jaskowiak-2011-comparing-correlation-coefficients-as-dissimilarity-measures-for-cancer-classification-in-gene-expression-data,
  title={Comparing correlation coefficients as dissimilarity measures for cancer classification in gene expression data},
  author={Jaskowiak, Pablo A and Campello, RJGB},
  booktitle={Proceedings of the Brazilian Symposium on Bioinformatics},
  pages={1--8},
  year={2011},
  organization={Bras{\'\i}lia},
  abstract={An important analysis performed in gene expression data is sample classification, e.g., the classification of different types or subtypes of cancer. Different classifiers have been employed for this challenging task, among which the k-Nearest Neighbors (kNN) classifier stands out for being at the same time very simple and highly flexible in terms of discriminatory power. Although the choice of a dissimilarity measure is essential to kNN, little effort has been undertaken to evaluate how this choice affects its performance in cancer classification. To this extent, we compare seven correlation coefficients for cancer classification using kNN. Our comparison suggests that a recently introduced correlation may perform better than commonly used measures. We also show that correlation coefficients rarely considered can provide competitive results when compared to widely used dissimilarity measures.}
}

@Article{coomans-1982-alternative-k-nearest-neighbour-rules-in-supervised-pattern-recognition-part-1-k-nearest-neighbour-classification-by-using-alternative-voting-rules,
  author = {Coomans, D. and Massart, D.L.},
  title = {Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. k-Nearest neighbour classification by using alternative voting rules},
  journal = {Analytica Chimica Acta},
  volume = {136},
  pages = {15 - 27},
  year = {1982},
  issn = {0003-2670},
  doi = {https://doi.org/10.1016/S0003-2670(01)95359-0},
  url = {http://www.sciencedirect.com/science/article/pii/S0003267001953590},
  abstract = {This paper discusses an extension of the well known k-nearest neighbour method. The majority voting procedure is replaced by an alternative voting method. This alternative kNN method is approached from both probabilistic and non-probabilistic points of view. On the basis of an example of differentiation between EU thyroid function and HYPER thyroid function, it is shown that alternative votes can give rise to better classification results than the majority vote.}
}

@Article{samworth-2012-optimal-weighted-nearest-neighbour-classifiers,
  author = {Samworth, Richard J.},
  doi = {10.1214/12-AOS1049},
  fjournal = {The Annals of Statistics},
  journal = {Ann. Statist.},
  month = {10},
  number = {5},
  pages = {2733--2763},
  publisher = {The Institute of Mathematical Statistics},
  title = {Optimal weighted nearest neighbour classifiers},
  url = {https://doi.org/10.1214/12-AOS1049},
  volume = {40},
  year = {2012},
  abstract={We derive an asymptotic expansion for the excess risk (regret) of a weighted nearest-neighbour classifier. This allows us to find the asymptotically optimal vector of nonnegative weights, which has a rather simple form. We show that the ratio of the regret of this classifier to that of an unweighted k-nearest neighbour classifier depends asymptotically only on the dimension d of the feature vectors, and not on the underlying populations. The improvement is greatest when d = 4, but thereafter decreases as d →∞. The popular bagged nearest neighbour classifier can also be regarded as a weighted nearest neighbour classifier, and we show that its corresponding weights are somewhat suboptimal when d is small (in particular, worse than those of the unweighted k-nearest neighbour classifier when d = 1), but are close to optimal when d is large. Finally, we argue that improvements in the rate of convergence are possible under stronger smoothness assumptions, provided we allow negative weights. Our findings are supported by an empirical performance comparison on both simulated and real data sets.}
}
