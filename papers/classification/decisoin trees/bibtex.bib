@Article{loh-2014-fifty-years-of-classification-and-regression-trees,
  author = {Loh, Wei-Yin},
  title = {Fifty Years of Classification and Regression Trees},
  journal = {International Statistical Review},
  volume = {82},
  number = {3},
  pages = {329-348},
  keywords = {Classification trees, regression trees, machine learning, prediction},
  doi = {10.1111/insr.12016},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12016},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12016},
  year={2014},
  
  abstract = {AbstractFifty years have passed since the publication of the first regression tree algorithm. New techniques have added capabilities that far surpass those of the early methods. Modern classification trees can partition the data with linear splits on subsets of variables and fit nearest neighbor, kernel density, and other models in the partitions. Regression trees can fit almost every kind of traditional statistical model, including least-squares, quantile, logistic, Poisson, and proportional hazards models, as well as models for longitudinal and multiresponse data. Greater availability and affordability of software (much of which is free) have played a significant role in helping the techniques gain acceptance and popularity in the broader scientific community. This article surveys the developments and briefly reviews the key ideas behind some of the major algorithms.}
}

@Article{zacharis-2018-classification-and-regression-trees-(cart)-for-predictive-modeling-in-blended-learning,
  author={Zacharis,Nick Z.},
  year={2018},
  month={03},
  title={Classification and Regression Trees (CART) for Predictive Modeling in Blended Learning},
  journal={International Journal of Intelligent Systems and Applications},
  volume={10},
  number={3},
  pages={1},
  note={Copyright - Copyright Modern Education and Computer Science Press Mar 2018; Last updated - 2018-03-27},

  abstract={Today, Internet and Web technologies not only provide students opportunities for flexible interactivity with study materials, peers and instructors, but also generate large amounts of usage data that can be processed and reveal behavioral patterns of study and learning. This study analyzed data extracted from a Moodle-based blended learning course, to build a student model that predicts course performance. CART decision tree algorithm was used to classify students and predict those at risk, based on the impact of four online activities: message exchanging, group wiki content creation, course files opening and online quiz taking. The overall percentage of correct classifications was about 99.1%, proving the model sensitive to identify very specific groups at risk.},

  keywords={Computers--Computer Systems; Education Data Mining; Student Data; Blended learning; Decision Trees; CART algorithm; Moodle; Computer assisted instruction; Mathematical models; Regression analysis; Learning; Distance education; Instructors; Students},
  isbn={2074904X},
  language={English},
  url={http://search.proquest.com.jproxy.lib.ecu.edu/docview/2018704816?accountid=10639},
}

@Article{chrysos-2013-hc-cart-a-parallel-system-implementation-of-data-mining-classification-and-regression-tree-cart-algorithm-on-a-multi-fpga-system,
  author = {Chrysos, Grigorios and Dagritzikos, Panagiotis and Papaefstathiou, Ioannis and Dollas, Apostolos},
  title = {HC-CART: A Parallel System Implementation of Data Mining Classification and Regression Tree (CART) Algorithm on a multi-FPGA System},
  journal = {ACM Trans. Archit. Code Optim.},
  volume = {9},
  number = {4},
  month = jan,
  year = {2013},
  issn = {1544-3566},
  pages = {47:1--47:25},
  articleno = {47},
  numpages = {25},
  url = {http://doi.acm.org/10.1145/2400682.2400706},
  doi = {10.1145/2400682.2400706},
  acmid = {2400706},
  publisher = {ACM},
  address = {New York, NY, USA},
 
  keywords = {Decision tree classification (DTC), HW architecture, R project, WEKA platform, classification and regression tree (CART), high-performance computing, reconfigurable system, rpart library},
 
  abstract={Data mining is a new field of computer science with a wide range of applications. Its goal is to extract knowledge from massive datasets in a human-understandable structure, for example, the decision trees. In this article we present an innovative, high-performance, system-level architecture for the Classification And Regression Tree (CART) algorithm, one of the most important and widely used algorithms in the data mining area. Our proposed architecture exploits parallelism at the decision variable level, and was fully implemented and evaluated on a modern high-performance reconfigurable platform, the Convey HC-1 server, that features four FPGAs and a multicore processor. Our FPGA-based implementation was integrated with the widely used “rpart” software library of the R project in order to provide the first fully functional reconfigurable system that can handle real-world large databases. The proposed system, named HC-CART system, achieves a performance speedup of up to two orders of magnitude compared to well-known single-threaded data mining software platforms, such as WEKA and the R platform. It also outperforms similar hardware systems which implement parts of the complete application by an order of magnitude. Finally, we show that the HC-CART system offers higher performance speedup than some other proposed parallel software implementations of decision tree construction algorithms.}
} 

@Article{mu-2016-a-parallel-c4.5-decision-tree-algorithm-based-on-mapreduce,
  author = {Mu, Yashuang and Liu, Xiaodong and Yang, Zhihao and Liu, Xiaolin},
  title = {A parallel C4.5 decision tree algorithm based on MapReduce},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {29},
  number = {8},
  pages = {e4015},
  keywords = {C4.5, decision trees, MapReduce, parallel computing},
  doi = {10.1002/cpe.4015},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.4015},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.4015},
  note = {e4015 cpe.4015},
  year={2016},

  abstract = {Summary In the supervised classification, large training data are very common, and decision trees are widely used. However, as some bottlenecks such as memory restrictions, time complexity, or data complexity, many supervised classifiers including classical C4.5 tree cannot directly handle big data. One solution for this problem is to design a highly parallelized learning algorithm. Motivated by this, we propose a parallelized C4.5 decision tree algorithm based on MapReduce (MR-C4.5-Tree) with 2 parallelized methods to build the tree nodes. First, an information entropy-based parallelized attribute selection method (MR-A-S) on several subsets for MR-C4.5-Tree is proposed to confirm the best splitting attribute and the cut points. Then, a data splitting method (MR-D-S) in parallel is presented to partition the training data into subsets. At last, we introduce the MR-C4.5-Tree learning algorithm that grows in a top-down recursive way. Besides, the depth of the constructed decision tree, the number of samples and the maximal class probability in each tree node are used as the termination conditions to avoid the over-partitioning problem. Experimental studies show the feasibility and the good performance of the proposed parallelized MR-C4.5-Tree algorithm.}
}

@Article{zhuang-2017-feature-bundling-in-decision-tree-algorithm,
  author = {Zhuang, Xu and Zhu, Yan and Chang, Chin-Chen and Peng, Qiang},
  issn = {1088467X},
  journal = {Intelligent Data Analysis},
  keywords = {Algorithms, Decision making, Tree graphs, Data modeling, Frame bundles, Decision tree, feature bundling, feature   transformation, web spam detection},
  number = {2},
  pages = {371 - 383},
  title = {Feature bundling in decision tree algorithm.},
  volume = {21},
  year = {2017},
  url = {http://jproxy.lib.ecu.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true&db=bth&AN=121698323&site=ehost-live&scope=site},

  abstract = {In empirical data modelling, a model of system is built up from a set of cases that the system has observed. Eventually, the performance of the inducted model is dominated by the quality and quantity of observations. Feature transformation methods are widely used to improve quality of knowledge extracted from observations to build up more accurate and robust model. In the paper, a new feature transformation method named dynamical feature bundling for decision tree algorithm is proposed. Dynamical feature bundling groups a set of features in the tree induction phase and it enables decision tree algorithms to 1) make use of features in one bundle together to make collective judgments in splitting phase; 2) learn more reliable and stable knowledge from feature bundles created based on domain knowledge of experts; 3) embed feature transformation step into tree induction phase, and therefore the extra pre-process step which are necessary for static feature transformation methods is inessen},
}

@INPROCEEDINGS{dumont-2009-fast-multi-class-image-annotation-with-random-subwindows-and-multiple-output-randomized-trees,
  title={Fast multi-class image annotation with random subwindows and multiple output randomized trees},
  author={Dumont, Marie and Mar{\'e}e, Rapha{\"e}l and Wehenkel, Louis and Geurts, Pierre},
  booktitle={Proc. International Conference on Computer Vision Theory and Applications (VISAPP)},
  volume={2},
  pages={196--203},
  year={2009},

  abstract={This paper addresses image annotation, i.e. labelling pixels of an image with a class among a finite set of predefined classes. We propose a new method which extracts a sample of subwindows from a set of annotated images in order to train a subwindow annotation model by using the extremely randomized trees ensemble method appropriately extended to handle high-dimensional output spaces. The annotation of a pixel of an unseen image is done by aggregating the annotations of its subwindows containing this pixel. The proposed method is compared to a more basic approach predicting the class of a pixel from a single window centered on that pixel and to other state-of-the-art image annotation methods. In terms of accuracy, the proposed method significantly outperforms the basic method and shows good performances with respect to the state-of-the-art, while being more generic, conceptually simpler, and of higher computational efficiency than these latter.}
}

@article{cherfi-2018-very-fast-c4.5-decision-tree-algorithm,
  author = {Cherfi, Anis and Nouira, Kaouther and Ferchichi, Ahmed},
  title = {Very Fast C4.5 Decision Tree Algorithm},
  journal = {Applied Artificial Intelligence},
  volume = {32},
  number = {2},
  pages = {119-137},
  year  = {2018},
  publisher = {Taylor & Francis},
  doi = {10.1080/08839514.2018.1447479},

  URL = {https://doi.org/10.1080/08839514.2018.1447479 },
  eprint = { https://doi.org/10.1080/08839514.2018.1447479},

  abstract = { ABSTRACTThis paper presents a novel algorithm so-called VFC4.5 for building decision trees. It proposes an adaptation of the way C4.5 finds the threshold of a continuous attribute. Instead of finding the threshold that maximizes gain ratio, the paper proposes to simply reduce the number of candidate cut points by using arithmetic mean and median to improve a reported weakness of the C4.5 algorithm when it deals with continuous attributes. This paper will focus primarily on the theoretical aspects of the VFC4.5 algorithm. An empirical trials, using 49 datasets, show that, in most times, the VFC4.5 algorithm leads to smaller decision trees with better accuracy compared to the C4.5 algorithm. VFC4.5 gives excellent accuracy results as C4.5 and it is much faster than the VFDT algorithm. }
}

@INPROCEEDINGS{freund-1999-the-alternating-decision-tree-learning-algorithm,
  title={The alternating decision tree learning algorithm},
  author={Freund, Yoav and Mason, Llew},
  booktitle={icml},
  volume={99},
  pages={124--133},
  year={1999},

  abstract={The application of boosting procedures to decision tree algorithms has been shown to produce very accurate classifiers. These classifiers are in the form of majority vote over a number of decision trees. Unfortunately, these classifiers are often large, complex and difficult to interpret. This paper describes a new type of classification rule, the alternating decision tree, which is a generalization of decision trees, voted decision trees and voted decision stumps. At the same time classifiers of this type are relatively easy to interpret. We present a learning algorithm for alternating decision trees that is based on boosting. Experimental results show it is competitive with boosted decision tree algorithms such as C5.0, and generates rules that are usually smaller in size and thus easier to interpret. In addition these rules yield a natural measure of classification confidence which can be used to improve the accuracy at the cost of abstaining from predicting examples that are hard to classify. }
}
