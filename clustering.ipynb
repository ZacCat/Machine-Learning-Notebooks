{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [4 General Functions (Review)](#general_functions)\n",
    "    * [4.1 Functions to Save and Open Variables](#open_save)\n",
    "* [5 Document Clustering](#document_clustering) \n",
    "    * [5.1 k-Means Clustering](#k_means)\n",
    "    * [5.2 Density-based Spatial Clustering of Applications with Noise (DBSCAN)](#dbscan)\n",
    "    * [5.3 Balanced Iterative Reducing and Clustering using Hierarchies (Birch)](#birch)\n",
    "    * [5.4 Affinity Propagation](#prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions <a id='general_functions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Save and Open Variables <a id='open_save'></a>\n",
    "\n",
    "Since it is not uncommon for a machine learning task to take a long time it is good practice to save variables that may be needed in the future. This can be achieved by using the [pickle](https://docs.python.org/3/library/pickle.html) module. This package allows a variable up to 4gb to be saved. This limitation is why the 'metrics' variables are saved as individual items instead of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save variables to file\n",
    "import pickle\n",
    "\n",
    "def save_var(variable_name):\n",
    "    \"\"\" Saves the variable with the provided variable name \n",
    "         in the global namespace to the ./vars folder \n",
    "         with the provided same name \"\"\"\n",
    "    \n",
    "    with open('./vars/' + variable_name,'wb') as my_file_obj:\n",
    "        pickle.dump(globals()[variable_name], my_file_obj, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_var_list(variable_name_list):\n",
    "    \"\"\" Saves each variable with the provided variable name \n",
    "         in the global namespace to the ./vars folder \n",
    "         with the provided same name \"\"\"\n",
    "    for name in variable_name_list:\n",
    "        with open('./vars/' + name,'wb') as my_file_obj:\n",
    "            pickle.dump(globals()[name], my_file_obj, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def open_var(file_name):\n",
    "    \"\"\" Returns the variable saved with the provided \n",
    "         file name located in the ./vars folder\"\"\"\n",
    "    \n",
    "    file_object = open('./vars/' + file_name,'rb')  \n",
    "\n",
    "    loaded_var = pickle.load(file_object)\n",
    "    \n",
    "    return loaded_var\n",
    "\n",
    "def open_var_list(file_name_list):\n",
    "    \"\"\" Loads a variable corresponding to each file name\n",
    "         in file_name_list to the global namespace. \"\"\"\n",
    "    \n",
    "    for file_name in file_name_list:\n",
    "        globals()[file_name] = open_var(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 157 ms, sys: 622 ms, total: 779 ms\n",
      "Wall time: 2.14 s\n"
     ]
    }
   ],
   "source": [
    "%time mnist = open_var('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.7 ms, sys: 959 ms, total: 990 ms\n",
      "Wall time: 3.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load Datasets\n",
    "class Dataset_Part:\n",
    "    \"\"\" Represents a dataset with attributes\n",
    "         data and target \"\"\"\n",
    "    \n",
    "    data = None\n",
    "    target = None\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "\n",
    "open_var_list(['mnist_train', 'mnist_test', 'rcv1_train', 'rcv1_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_digit(dataset, index):\n",
    "    # Get a random document\n",
    "    digit_arr = dataset.data[index]\n",
    "    # Reshape it to the size of the image\n",
    "    digit_image = digit_arr.reshape(28,28)\n",
    "\n",
    "    # Some information\n",
    "    print(f'\\tIndex: {index}\\tLabel: {dataset.target[index]:.0f}')\n",
    "    # Show the image\n",
    "    plt.imshow(digit_image, cmap=matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Document Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) <a id='document_clustering'></a>\n",
    "\n",
    "Clustering is an unsupervised training method, meaning it is performed on data without labels. Because of this unsupervised learning is capable of finding relations that may not have been previously observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Cluster Evaluation]() <a id='cluster_evaluation'></a>\n",
    "\n",
    "Unsupervised learning uses different evaluation metrics than supervised learning. This is because unsupervised learning makes assumptions with no prior knowladge (ie. no labels). Since the data does not conform to predetermined labels evaluation metrics such as precision and recall cannot be performed. Instead the following metrics can be used.\n",
    "\n",
    "+ __homogeneity score__ - \n",
    "+ __completeness score__ -\n",
    "+ __silhouette score__ - \n",
    "+ __V-measure__ - The harmonic mean between homogeneity and completeness.\n",
    "$$ v = 2 \\cdot \\frac{ \\text{homogeneity} \\cdot \\text{completeness}}{\\text{homogeneity} + \\text{completeness}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "from time import time\n",
    "\n",
    "def bench_clust(estimator_lst, name_lst, data, labels):\n",
    "\n",
    "    print('%-9s\\t%-6s\\t%-12s\\t%-4s\\t%-4s\\t%-4s\\t%-4s\\t%-4s\\t%-4s' \n",
    "          % ( 'title', 'time', 'inertia', 'homog', 'comp', 'v mes',\n",
    "              'rand', 'mutu', 'silh'))\n",
    "    \n",
    "    for estimator, name in zip(estimator_lst, name_lst):\n",
    "        t0 = time()\n",
    "        estimator.fit(data)\n",
    "        print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "              % (name, (time() - t0), .0, #estimator.inertia_,\n",
    "                 metrics.homogeneity_score(labels, estimator.labels_),\n",
    "                 metrics.completeness_score(labels, estimator.labels_),\n",
    "                 metrics.v_measure_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_mutual_info_score(labels,  estimator.labels_), 0.))\n",
    "#              metrics.silhouette_score(data, estimator.labels_,\n",
    "#                                       metric='euclidean',\n",
    "#                                       sample_size=1000)))\n",
    "#     return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [k-Means Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) <a id='k_means'></a>\n",
    "\n",
    "This algorithm is implemented in the sklearn.cluster.KMeans scikit-learn module. K-means clustering attempts to seperate data into a predetermined, k, number of clusters. The aim is to create clusters with equal variance, thus minimizing inertia, also known as the within-cluster sum of squares. Inertia is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    " \\sum_{i=0}^n \\min_{\\mu_j \\in C} (\\mid \\mid x_j - \\mu_i \\mid \\mid^2)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "[comment]: <> (need to reword, too close to source)\n",
    "To find clusters k-Means has a three step process explained by [Zhao et al.](https://doi.org/10.1016/j.neucom.2018.02.072) [1] as:\n",
    "\n",
    "1. Initialize k centroids, one for each cluster. The most basic way to do this is by picking k random samples.\n",
    "+ Assign each sample to the closest centroid.\n",
    "+ Recompute centroids with assignments from previous step.\n",
    "+ Repeat step 2 and step 3 until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title    \ttime  \tinertia     \thomog\tcomp\tv mes\trand\tmutu\tsilh\n",
      "k-means++ k=3\t20.23s\t213604851206\t0.211\t0.443\t0.286\t0.172\t0.211\t0.057\n",
      "k-means++ k=5\t24.08s\t197606838359\t0.390\t0.578\t0.465\t0.330\t0.390\t0.072\n",
      "k-means++ k=10\t32.67s\t178462843731\t0.482\t0.485\t0.484\t0.362\t0.482\t0.056\n",
      "random k=10\t34.80s\t178432572697\t0.496\t0.504\t0.500\t0.367\t0.496\t0.059\n",
      "k-means++ k=15\t45.41s\t167326445668\t0.581\t0.499\t0.537\t0.379\t0.499\t0.062\n",
      "random k=15\t49.65s\t167326155420\t0.581\t0.499\t0.537\t0.379\t0.499\t0.071\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from time import time\n",
    "\n",
    "def bench_k_means(estimator_lst, name_lst, data, labels):\n",
    "    print('%-9s\\t%-6s\\t%-12s\\t%-4s\\t%-4s\\t%-4s\\t%-4s\\t%-4s\\t%-4s' \n",
    "      % ( 'title', 'time', 'inertia', 'homog', 'comp', 'v mes', 'rand', 'mutu', 'silh'))\n",
    "    for estimator, name in zip(estimator_lst, name_lst):\n",
    "        t0 = time()\n",
    "        estimator.fit(data)\n",
    "        print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "              % (name, (time() - t0), estimator.inertia_,\n",
    "                 metrics.homogeneity_score(labels, estimator.labels_),\n",
    "                 metrics.completeness_score(labels, estimator.labels_),\n",
    "                 metrics.v_measure_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "                 metrics.silhouette_score(data, estimator.labels_,\n",
    "                                          metric='euclidean',\n",
    "                                          sample_size=1000)))\n",
    "\n",
    "bench_k_means([KMeans(init='k-means++', n_clusters=3, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='k-means++', n_clusters=5, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='k-means++', n_clusters=10, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='random', n_clusters=10, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='k-means++', n_clusters=15, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='random', n_clusters=15, n_init=10, n_jobs=-1) ],\n",
    "              [\"k-means++ k=3\", \"k-means++ k=5\", \"k-means++ k=10\", \"random k=10\", \"k-means++ k=15\", \"random k=15\"],\n",
    "              data=mnist.data, labels=mnist.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 µs, sys: 4 µs, total: 18 µs\n",
      "Wall time: 22.2 µs\n",
      "CPU times: user 2.41 s, sys: 376 ms, total: 2.78 s\n",
      "Wall time: 16.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=10, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "    random_state=23, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clf_kmeans = KMeans(n_clusters=10, random_state=23, n_jobs=-1)\n",
    "clf_kmeans.fit(mnist_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pred_kmeans = kmeans.predict(mnist_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores_kmeans = get_scores(mnist_test.target, pred_kmeans, 'macro')\n",
    "print_scores(scores_kmeans, 'macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Density-based Spatial Clustering of Applications with Noise (DBSCAN)](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) <a id='dbscan'></a>\n",
    "\n",
    "The DBSCAN algorithm clusters samples into areas of high density with surrounding low dennsity areas. Because of this Clusters can be any shape and the number of clusters is not predeturmined. Clusters are formed by finding region that satisfy a minimum density, number of documents per area. The shape of the cluster is determined by the distance metric used. Any distance function can be used and the distance function will determine the shape of the clusters [2].\n",
    "\n",
    "To form a cluster DBSCAN searches for areas with a minimum number of points within a specified distance, $\\varepsilon$, from a central point, this area is called an $\\varepsilon$-neighborhood. Each point in a $\\varepsilon$-neighborhood will expand outward, and if this neighborhood meets the minimum number of points required the cluster is updated to include this $\\varepsilon$-neighborhood. Points that are not within $\\varepsilon$ of the center, but it is included in the cluster it is said to be density-reachable. [Good visualization here](https://cse.buffalo.edu/~jing/cse601/fa12/materials/clustering_density.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "ones = np.where(mnist.target == 2.)[0]\n",
    "result = []\n",
    "tot_result = 0.\n",
    "count = 0\n",
    "# result = np.linalg.norm(mnist.data[ones], 'fro')\n",
    "for k in ones:\n",
    "    for i in ones[random.sample(range(len(ones)), 100)]:\n",
    "        if k != i:\n",
    "            res = np.linalg.norm(mnist.data[i] - mnist.data[k])\n",
    "            result += [res]\n",
    "            tot_result += res\n",
    "            count += 1\n",
    "\n",
    "avg_result = tot_result / count\n",
    "avg_result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('eps, samp       time      inertia       homo    comp    v mes   rand   mutual   silh')\n",
    "clf_dbscan = bench_clust(DBSCAN(eps=10, min_samples=10, n_jobs=-1),name=\"10, 10\", data=mnist.data, labels=mnist.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "clf_dbscan = DBSCAN(eps=10, min_samples=10, n_jobs=-1)\n",
    "clf_dbscan.fit(mnist_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clf_dbscan_auto = DBSCAN(n_jobs=-1)\n",
    "clf_dbscan_auto.fit(mnist_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title    \ttime  \tinertia     \thomog\tcomp\tv mes\trand\tmutu\tsilh\n",
      "auto     \t1186.30s\t0\t-0.000\t1.000\t-0.000\t0.000\t-0.000\t0.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "bench_clust([DBSCAN(n_jobs=-1)] ,[\"auto\"], data=mnist.data, labels=mnist.target)\n",
    "# bench_clust(clf_dbscan ,name=\"10, 10\", data=mnist_test.data, labels=mnist_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Balanced Iterative Reducing and Clustering using Hierarchies (Birch)](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html) <a id='birch'></a>\n",
    "\n",
    "[Paper](https://rdcu.be/XdFp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Affinity Propagation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation) <a id='prop'></a>\n",
    "\n",
    "Affinity propagation uses exemplars instead of centroids. This means that instead of finding a centroid affinity propagation works by finding samples that are most representative of the other samples. In addition the number of clusters is not predetermined. The number of clusters is determined by the data. \n",
    "\n",
    "[comment]: <> (need to reword, too close to source)\n",
    "There are three main formulas used in affinity propagation.\n",
    "\n",
    "1. The similarity of two samples is denoted $s(i,k)$.\n",
    "2. The responsibility of a sample, $k$, to be an exemplar of sample, $i$. \n",
    "  \n",
    " \\begin{equation*} \n",
    "   r(i, k) \\leftarrow s(i,k) - \\max [a(i, k') + s(i, k') \\forall k' \\neq k] \n",
    " \\end{equation*}\n",
    "\n",
    "3. The accumulated evidence that sample $i$ should choose sample $k$ to be its exemplar.\n",
    " \n",
    " \\begin{equation*}\n",
    "   a(i, k) \\leftarrow \\min [0, r(k,k) + \\sum_{i' ~ s.t.~ i' \\notin \\{i,k\\}} r(i',k)]\n",
    " \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibtex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K means article [1]\n",
    "@article{ZHAO2018195,\n",
    "title = \"k-means: A revisit\",\n",
    "journal = \"Neurocomputing\",\n",
    "volume = \"291\",\n",
    "pages = \"195 - 206\",\n",
    "year = \"2018\",\n",
    "issn = \"0925-2312\",\n",
    "doi = \"https://doi.org/10.1016/j.neucom.2018.02.072\",\n",
    "url = \"http://www.sciencedirect.com/science/article/pii/S092523121830239X\",\n",
    "author = \"Wan-Lei Zhao and Cheng-Hao Deng and Chong-Wah Ngo\",\n",
    "keywords = \"Clustering, -means, Incremental optimization\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN article [2] \n",
    "@inproceedings{Ester:1996:DAD:3001460.3001507,\n",
    " author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\\\"{o}rg and Xu, Xiaowei},\n",
    " title = {A Density-based Algorithm for Discovering Clusters a Density-based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},\n",
    " booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},\n",
    " series = {KDD'96},\n",
    " year = {1996},\n",
    " location = {Portland, Oregon},\n",
    " pages = {226--231},\n",
    " numpages = {6},\n",
    " url = {http://dl.acm.org/citation.cfm?id=3001460.3001507},\n",
    " acmid = {3001507},\n",
    " publisher = {AAAI Press},\n",
    " keywords = {arbitrary shape of clusters, clustering algorithms, efficiency on large spatial databases, handling nlj4-275oise},\n",
    " } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V-Measuer article [3]\n",
    "\n",
    "@inproceedings{rosenberg2007v,\n",
    "  title={V-measure: A conditional entropy-based external cluster evaluation measure},\n",
    "  author={Rosenberg, Andrew and Hirschberg, Julia},\n",
    "  booktitle={Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL)},\n",
    "  year={2007},\n",
    "  url = {http://aclweb.org/anthology/D/D07/D07-1043.pdf},\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
