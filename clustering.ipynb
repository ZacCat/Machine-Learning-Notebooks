{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [4 General Functions (Review)](#general_functions)\n",
    "    * [4.1 Functions to Save and Open Variables](#open_save)\n",
    "* [5 Document Clustering](#document_clustering) \n",
    "    * [5.1 k-Means Clustering](#k_means)\n",
    "    * [5.2 Density-based Spatial Clustering of Applications with Noise (DBSCAN)](#dbscan)\n",
    "    * [5.3 Balanced Iterative Reducing and Clustering using Hierarchies (Birch)](#birch)\n",
    "    * [5.4 Affinity Propagation](#prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions <a id='general_functions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Save and Open Variables <a id='open_save'></a>\n",
    "\n",
    "Since it is not uncommon for a machine learning task to take a long time it is good practice to save variables that may be needed in the future. This can be achieved by using the [pickle](https://docs.python.org/3/library/pickle.html) module. This package allows a variable up to 4gb to be saved. This limitation is why the 'metrics' variables are saved as individual items instead of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOBLIB_TEMP_FOLDER=/home/jovyan/work/tmp\n"
     ]
    }
   ],
   "source": [
    "%env JOBLIB_TEMP_FOLDER=/home/jovyan/work/tmp\n",
    "\n",
    "# Save variables to file\n",
    "import pickle\n",
    "\n",
    "def save_var(variable_name):\n",
    "    \"\"\" Saves the variable with the provided variable name \n",
    "         in the global namespace to the ./vars folder \n",
    "         with the provided same name \"\"\"\n",
    "    \n",
    "    with open('./vars/' + variable_name,'wb') as my_file_obj:\n",
    "        pickle.dump(globals()[variable_name], my_file_obj, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_var_list(variable_name_list):\n",
    "    \"\"\" Saves each variable with the provided variable name \n",
    "         in the global namespace to the ./vars folder \n",
    "         with the provided same name \"\"\"\n",
    "    for name in variable_name_list:\n",
    "        with open('./vars/' + name,'wb') as my_file_obj:\n",
    "            pickle.dump(globals()[name], my_file_obj, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def open_var(file_name):\n",
    "    \"\"\" Returns the variable saved with the provided \n",
    "         file name located in the ./vars folder\"\"\"\n",
    "    \n",
    "    file_object = open('./vars/' + file_name,'rb')  \n",
    "\n",
    "    loaded_var = pickle.load(file_object)\n",
    "    \n",
    "    return loaded_var\n",
    "\n",
    "def open_var_list(file_name_list):\n",
    "    \"\"\" Loads a variable corresponding to each file name\n",
    "         in file_name_list to the global namespace. \"\"\"\n",
    "    \n",
    "    for file_name in file_name_list:\n",
    "        globals()[file_name] = open_var(file_name)\n",
    "        \n",
    "mnist = open_var('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 ms, sys: 76 ms, total: 92 ms\n",
      "Wall time: 93.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load Datasets\n",
    "class Dataset_Part:\n",
    "    \"\"\" Represents a dataset with attributes\n",
    "         data and target \"\"\"\n",
    "    \n",
    "    data = None\n",
    "    target = None\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "\n",
    "open_var_list(['mnist_train', 'mnist_test', 'rcv1_train', 'rcv1_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_digit(dataset, index):\n",
    "    # Get a random document\n",
    "    digit_arr = dataset.data[index]\n",
    "    # Reshape it to the size of the image\n",
    "    digit_image = digit_arr.reshape(28,28)\n",
    "\n",
    "    # Some information\n",
    "    print(f'\\tIndex: {index}\\tLabel: {dataset.target[index]:.0f}')\n",
    "    # Show the image\n",
    "    plt.imshow(digit_image, cmap=matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Document Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) <a id='document_clustering'></a>\n",
    "\n",
    "Clustering is an unsupervised training method, meaning it is performed on data without labels. Because of this unsupervised learning is capable of finding relations that may not have been previously observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Cluster Evaluation](http://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure) <a id='cluster_evaluation'></a>\n",
    "\n",
    "Unsupervised learning uses different evaluation metrics than supervised learning. This is because unsupervised learning makes assumptions with no prior knowladge (ie. no labels). Since the data does not conform to predetermined labels evaluation metrics such as precision and recall cannot be performed. Instead the following metrics can be used.\n",
    "\n",
    "+ [__homogeneity score__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html#sklearn.metrics.homogeneity_score) - A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class. A value of 1.0 represents perfectly homogenious labeling. Shown in equation (1) from [rosenberg-2007].\n",
    "$$ h = 1 - \\frac{H(C \\mid K)}{H(C)} $$   \n",
    "$$H(C \\mid K) = - \\sum^{\\mid C \\mid}_{c=1} \\sum^{\\mid K \\mid}_{k=1} \\frac{a_{c,k}}{N} \\cdot \\log \\frac{a_{c,k}}{\\sum^{\\mid C \\mid}_{c=1} a_{c,k}} $$  \n",
    "$$H(C) = - \\sum^{\\mid C \\mid}_{c=1} \\frac{\\sum^{\\mid K \\mid}_{k=1} a_{c,k}}{n} \\cdot \\log \\frac{\\sum^{\\mid K \\mid}_{k=1} a_{c,k}}{n} $$\n",
    "+ [__completeness score__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html) - A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster. A value of 1.0 represents perfectly complete labeling. Shown in equation (1) from [rosenberg-2007].\n",
    "$$ c = 1 - \\frac{H(K \\mid C)}{H(K)} $$\n",
    "+ [__V-measure__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html) - V-measure is similar to F1 score in the sense that it is a harmonic mean, except it relates homogeneity and completeness [rosenberg-2007].  \n",
    "$$ v = 2 \\cdot \\frac{ \\text{homogeneity} \\cdot \\text{completeness}}{\\text{homogeneity} + \\text{completeness}} $$\n",
    "+ [__Rand index__](https://doi.org/10.1007/BF01908075) - Measure of similarity between the predicted and true clusters. Rand Index considers all pairs of samples and counts the number of pairs that are assigned correctly to the same cluster, incorrectly to the same cluster, correctly to seperate clusters, and incorrectly to different clusters. [hubert-1985]\n",
    "  + __TODO: PARAPHRASE ASSIGNMENT__\n",
    "  + If C is the ground truth of class assignment and K the clustering, let us define:\n",
    "    + a as the number of pairs of elements that are in the same set in C and in the same set in K\n",
    "    + b as the number of pairs of elements that are in different sets in C and in different sets in K\n",
    "    + $C^{n_{samples}}_2$ as the total number of possible pairs in the dataset (without ordering)\n",
    "$$ RI = \\frac{a + b}{C^{n_{samples}}_2}$$\n",
    "+ [__adjusted Rand score__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) - The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.\n",
    "$$ ARI = \\frac{RI - E(RI)}{\\max(RI) - E(RI)} $$\n",
    "  + Note - E(RI) means expected RI, or the RI given random labelings. \n",
    "+ [__mutual information score__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html) - The measure of similarity between the predicted and true labels, ignoring permutation. Given two sets of clusters $V$ and $U$. Suppose $U$ has size $i$, denoted as $\\mid U \\mid = i$, and similarly for $\\mid V \\mid = j$.\n",
    "$$ MI(U,V) = \\sum^{\\mid U \\mid}_{i=1} \\sum^{\\mid V \\mid}_{j=1} \\frac{\\mid U_i \\cap V_j \\mid}{N} \\cdot log ( \\frac{N \\mid U_i \\cap V_j \\mid}{ \\mid U_i \\mid \\mid V_j \\mid}) $$\n",
    "+ [__adjusted mutual info score__](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html) - The mutual information score adjusted to account for the fact tha mutual information score is typically greater when there are more clusters. The adjusted mutual information score is 1 when the two sets of clusters are the same. Random clustering have an expected adjusted mutual information score near 0. Shown in equation (3) from [vinh-2010].\n",
    "$$ AMI(U,V) = \\frac{MI(U,V) - E(MI(U, V))} {\\max(H(U), H(V)) - E(MI(U, V))} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from time import time\n",
    "from multiprocessing import Process, Manager\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def fit_pred(estimator, data, labels, t0, name, est_type):\n",
    "    \n",
    "    print(f'fitting {name} estimator', file=open(f'./output.txt', 'a'))\n",
    "    \n",
    "    estimator.fit(data)\n",
    "\n",
    "    print(f'finished fitting {name} estimator', file=open(f'./output.txt', 'a'))\n",
    "    \n",
    "    homo = metrics.homogeneity_score(labels, estimator.labels_)\n",
    "    comp = metrics.completeness_score(labels, estimator.labels_)\n",
    "    v_meas = metrics.v_measure_score(labels, estimator.labels_)\n",
    "    adj_rand = metrics.adjusted_rand_score(labels, estimator.labels_)\n",
    "    adj_mutu = metrics.adjusted_mutual_info_score(labels,  estimator.labels_)\n",
    "    n_labels = len(estimator.labels_)\n",
    "    \n",
    "    vals = f'{name:9s}\\t{(time() - t0):7.2f}s\\t{homo:.3f}\\t{comp:.3f}\\t{v_meas:.3f}\\t{adj_rand:.3f}\\t{adj_mutu:.3f}'\n",
    "\n",
    "    print(vals)\n",
    "    print(vals, file=open(f'./out/{est_type}.txt', 'a'))\n",
    "    \n",
    "    print(f'saving {name} estimator', file=open(f'./output.txt', 'a'))\n",
    "\n",
    "    joblib.dump(estimator, f'./vars/{name}.pkl')\n",
    "    \n",
    "    print(f'finished saving {name} estimator', file=open(f'./output.txt', 'a'))\n",
    "\n",
    "    \n",
    "def bench_clust(estimator_lst, name_lst, data, labels, est_type):\n",
    "\n",
    "    print('%-9s\\t   %-5s\\t%-4s\\t%-4s\\t%-4s\\t%-4s\\t%-4s' \n",
    "          % ( 'title', 'time', 'homog', 'comp', 'v mes',\n",
    "              'rand', 'mutu'))\n",
    "    \n",
    "    manager = Manager()\n",
    "    est_lst = manager.list()\n",
    "    \n",
    "    processes = []\n",
    "    for estimator, name in zip(estimator_lst, name_lst):\n",
    "        t0 = time()\n",
    "        \n",
    "        p = Process(target=fit_pred, args=(estimator, data, labels, t0, name, est_type))\n",
    "        p.start()\n",
    "        \n",
    "        processes += [p]\n",
    "\n",
    "        est_lst += [estimator]\n",
    "    return est_lst, processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [k-Means Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) <a id='k_means'></a>\n",
    "\n",
    "This algorithm is implemented in the sklearn.cluster.KMeans scikit-learn module. K-means clustering attempts to seperate data into a predetermined, k, number of clusters. The aim is to create clusters with equal variance, thus minimizing inertia, also known as the within-cluster sum of squares. Inertia is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    " \\sum_{i=0}^n \\min_{\\mu_j \\in C} (\\mid \\mid x_j - \\mu_i \\mid \\mid^2)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "[comment]: <> (need to reword, too close to source)\n",
    "To find clusters k-Means has a three step process explained by [Zhao et al.](https://doi.org/10.1016/j.neucom.2018.02.072) [zhao-2018] as:\n",
    "\n",
    "1. Initialize k centroids, one for each cluster. The most basic way to do this is by picking k random samples.\n",
    "+ Assign each sample to the closest centroid.\n",
    "+ Recompute centroids with assignments from previous step.\n",
    "+ Repeat step 2 and step 3 until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "def bench_k_means(estimator_lst, name_lst, data, labels):\n",
    "    print('%-9s\\t%-6s\\t%-12s\\t%-4s\\t%-4s\\t%-4s\\t%-4s\\t%-4s\\t%-4s' \n",
    "      % ( 'title', 'time', 'inertia', 'homog', 'comp', 'v mes', 'rand', 'mutu', 'silh'))\n",
    "    for estimator, name in zip(estimator_lst, name_lst):\n",
    "        t0 = time()\n",
    "        estimator.fit(data)\n",
    "        print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "              % (name, (time() - t0), estimator.inertia_,\n",
    "                 metrics.homogeneity_score(labels, estimator.labels_),\n",
    "                 metrics.completeness_score(labels, estimator.labels_),\n",
    "                 metrics.v_measure_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "                 metrics.silhouette_score(data, estimator.labels_,\n",
    "                                          metric='euclidean',\n",
    "                                          sample_size=1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title    \ttime  \tinertia     \thomog\tcomp\tv mes\trand\tmutu\tsilh\n",
      "k-means++ k=3\t17.52s\t213604832818\t0.211\t0.443\t0.286\t0.172\t0.211\t0.058\n",
      "k-means++ k=5\t18.71s\t197606834439\t0.390\t0.578\t0.466\t0.331\t0.390\t0.064\n",
      "k-means++ k=10\t27.39s\t178432593770\t0.496\t0.504\t0.500\t0.367\t0.496\t0.061\n",
      "random k=10\t24.52s\t178432235366\t0.496\t0.503\t0.500\t0.365\t0.496\t0.058\n",
      "k-means++ k=15\t27.78s\t167326324983\t0.581\t0.499\t0.537\t0.379\t0.499\t0.060\n",
      "random k=15\t32.69s\t167326474664\t0.580\t0.498\t0.536\t0.378\t0.498\t0.059\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "bench_k_means([KMeans(init='k-means++', n_clusters=3, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='k-means++', n_clusters=5, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='k-means++', n_clusters=10, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='random', n_clusters=10, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='k-means++', n_clusters=15, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='random', n_clusters=15, n_init=10, n_jobs=-1) ],\n",
    "              [\"k-means++ k=3\", \"k-means++ k=5\", \"k-means++ k=10\", \"random k=10\", \"k-means++ k=15\", \"random k=15\"],\n",
    "              data=mnist.data, labels=mnist.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title    \t   time \thomog\tcomp\tv mes\trand\tmutu\n",
      "k-means++ k=3\t  49.10s\t0.211\t0.443\t0.286\t0.172\t0.211\n",
      "k-means++ k=5\t  57.04s\t0.390\t0.578\t0.465\t0.331\t0.390\n",
      "random k=10\t  61.76s\t0.496\t0.503\t0.500\t0.365\t0.496\n",
      "k-means++ k=10\t  68.20s\t0.496\t0.504\t0.500\t0.366\t0.496\n",
      "k-means++ k=15\t  68.49s\t0.582\t0.499\t0.537\t0.380\t0.499\n",
      "random k=15\t  73.17s\t0.580\t0.498\t0.536\t0.378\t0.498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "est_km, proc_km = bench_clust(\n",
    "              [ KMeans(init='k-means++', n_clusters=3, n_init=10, n_jobs=-1),\n",
    "                KMeans(init='k-means++', n_clusters=5, n_init=10, n_jobs=-1),\n",
    "                KMeans(init='k-means++', n_clusters=10, n_init=10, n_jobs=-1),\n",
    "                KMeans(init='random', n_clusters=10, n_init=10, n_jobs=-1),\n",
    "                KMeans(init='k-means++', n_clusters=15, n_init=10, n_jobs=-1),\n",
    "                KMeans(init='random', n_clusters=15, n_init=10, n_jobs=-1) ],\n",
    "              [ \"k-means++ k=3\", \"k-means++ k=5\", \"k-means++ k=10\", \"random k=10\", \"k-means++ k=15\", \"random k=15\"],\n",
    "                data=mnist.data, labels=mnist.target, est_type='kMeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'proc_km' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2e833bea0d9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproc_km\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'proc_km' is not defined"
     ]
    }
   ],
   "source": [
    "for p in proc_km:\n",
    "    p.terminate()\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Density-based Spatial Clustering of Applications with Noise (DBSCAN)](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) <a id='dbscan'></a>\n",
    "\n",
    "The DBSCAN algorithm clusters samples into areas of high density with surrounding low dennsity areas. Because of this Clusters can be any shape and the number of clusters is not predeturmined. Clusters are formed by finding region that satisfy a minimum density, number of documents per area. The shape of the cluster is determined by the distance metric used. Any distance function can be used and the distance function will determine the shape of the clusters [ester-1996].\n",
    "\n",
    "To form a cluster DBSCAN searches for areas with a minimum number of points within a specified distance, $\\varepsilon$, from a central point, this area is called an $\\varepsilon$-neighborhood. Each point in a $\\varepsilon$-neighborhood will expand outward, and if this neighborhood meets the minimum number of points required the cluster is updated to include this $\\varepsilon$-neighborhood. Points that are not within $\\varepsilon$ of the center, but it is included in the cluster it is said to be density-reachable. [Good visualization here](https://cse.buffalo.edu/~jing/cse601/fa12/materials/clustering_density.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2422.3517784359951"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "ones = np.where(mnist.target == 2.)[0]\n",
    "result = []\n",
    "tot_result = 0.\n",
    "count = 0\n",
    "# result = np.linalg.norm(mnist.data[ones], 'fro')\n",
    "for k in ones:\n",
    "    for i in ones[random.sample(range(len(ones)), 100)]:\n",
    "        if k != i:\n",
    "            res = np.linalg.norm(mnist.data[i] - mnist.data[k])\n",
    "            result += [res]\n",
    "            tot_result += res\n",
    "            count += 1\n",
    "\n",
    "avg_result = tot_result / count\n",
    "avg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title    \t   time \thomog\tcomp\tv mes\trand\tmutu\n",
      "auto     \t1014.43s\t-0.000\t1.000\t-0.000\t0.000\t-0.000\n",
      "10,100   \t1639.32s\t-0.000\t1.000\t-0.000\t0.000\t-0.000\n",
      "10,1000  \t1656.99s\t-0.000\t1.000\t-0.000\t0.000\t-0.000\n",
      "100,10   \t2505.80s\t-0.000\t1.000\t-0.000\t0.000\t-0.000\n",
      "1000, 10 \t8758.63s\t0.142\t0.538\t0.225\t0.065\t0.141\n",
      "2000, 10 \t9785.88s\t0.000\t0.128\t0.000\t0.000\t0.000\n",
      "2000, 5  \t9793.47s\t0.000\t0.129\t0.000\t0.000\t0.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "est_db, proc=_db = bench_clust(\n",
    "          [ DBSCAN(n_jobs=-1),\n",
    "            DBSCAN(eps=1000, min_samples=10, n_jobs=-1),\n",
    "            DBSCAN(eps=100, min_samples=10, n_jobs=-1),\n",
    "            DBSCAN(eps=10, min_samples=100, n_jobs=-1),\n",
    "            DBSCAN(eps=10, min_samples=1000, n_jobs=-1),\n",
    "            DBSCAN(eps=2000, min_samples=5, n_jobs=-1),\n",
    "            DBSCAN(eps=2000, min_samples=10, n_jobs=-1) ],\n",
    "          [ \"auto\", \"1000,10\", \"100,10\", \"10,100\", \"10,1000\", \"2000,5\", \"2000,10\" ], \n",
    "                    data=mnist.data, labels=mnist.target, est_type='dbscan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title    \t   time \thomog\tcomp\tv mes\trand\tmutu\n",
      "500,20   \t5855.63s\t0.053\t0.604\t0.097\t0.008\t0.053\n",
      "500,10   \t5863.42s\t0.068\t0.664\t0.124\t0.014\t0.068\n",
      "500,5    \t5870.39s\t0.079\t0.684\t0.142\t0.018\t0.079\n",
      "500,50   \t5870.52s\t0.021\t0.396\t0.040\t0.000\t0.021\n",
      "750,50   \t7657.90s\t0.123\t0.873\t0.216\t0.041\t0.123\n",
      "750,5    \t7659.11s\t0.143\t0.804\t0.243\t0.047\t0.142\n",
      "750,10   \t7665.11s\t0.134\t0.873\t0.232\t0.045\t0.133\n",
      "750,20   \t7675.75s\t0.129\t0.896\t0.225\t0.044\t0.129\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "est_db, proc_db = bench_clust(\n",
    "          [ DBSCAN(eps=500, min_sample4s=50, n_jobs=-1),\n",
    "            DBSCAN(eps=500, min_samples=20, n_jobs=-1),\n",
    "            DBSCAN(eps=500, min_samples=10, n_jobs=-1),\n",
    "            DBSCAN(eps=500, min_samples=5, n_jobs=-1),\n",
    "            DBSCAN(eps=750, min_samples=50, n_jobs=-1),\n",
    "            DBSCAN(eps=750, min_samples=20, n_jobs=-1),\n",
    "            DBSCAN(eps=750, min_samples=10, n_jobs=-1),\n",
    "            DBSCAN(eps=750, min_samples=5, n_jobs=-1) ],\n",
    "          [ \"500,50\", \"500,20\",  \"500,10\", \"500,5\", \"750,50\", \"750,20\",  \"750,10\",  \"750,5\"], \n",
    "            data=mnist.data, labels=mnist.target, est_type='dbscan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in proc_db:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one of the best performing models is when eps=1000 min_samples=10. To further evaluate we need to look at how labels are being classified. A value of -1 means that this is considered a noisy sample, and 53,469 have been counted as noisy data. Also, there are 30 labels when there should be around 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1: 53469,\n",
       "         1: 135,\n",
       "         0: 74,\n",
       "         9: 10,\n",
       "         11: 11,\n",
       "         3: 17,\n",
       "         4: 39,\n",
       "         2: 8,\n",
       "         12: 7,\n",
       "         6: 5,\n",
       "         7: 9,\n",
       "         10: 14,\n",
       "         5: 9,\n",
       "         8: 10,\n",
       "         13: 15936,\n",
       "         14: 10,\n",
       "         15: 15,\n",
       "         16: 11,\n",
       "         27: 10,\n",
       "         28: 11,\n",
       "         29: 9,\n",
       "         17: 74,\n",
       "         18: 7,\n",
       "         19: 11,\n",
       "         20: 13,\n",
       "         23: 6,\n",
       "         22: 16,\n",
       "         21: 10,\n",
       "         24: 20,\n",
       "         31: 7,\n",
       "         25: 7,\n",
       "         26: 5,\n",
       "         30: 5})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "clf_db = joblib.load('./vars/1000,10.pkl')\n",
    "\n",
    "Counter(clf_db.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Affinity Propagation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html) <a id='prop'></a>\n",
    "\n",
    "Affinity propagation uses exemplars instead of centroids. This means that instead of finding a centroid affinity propagation works by finding samples that are most representative of the other samples. In addition the number of clusters is not predetermined. The number of clusters is determined by the data. \n",
    "\n",
    "There are three main formulas used in affinity propagation.\n",
    "\n",
    "1. The similarity, or Euclidean distance, between two samples $s(i,k)$. \n",
    "2. The responsibility, $r(i,k)$, represents \"the accumulated evidence for how well-suited point $k$ is to serve as the exemplar for point $i$, taking into account other potential exemplars for point $i$\" [frey-2007]. Equation (1) from [frey-2007]:\n",
    "  \n",
    " \\begin{equation*} \n",
    "   r(i, k) \\leftarrow s(i,k) - \\max_{k' s.t k' \\neq k} \\{a(i, k') + s(i, k')\\}\n",
    " \\end{equation*}\n",
    "\n",
    "3. The availability, a(i, k), represents \"the accumulated evidence for how appropriate it would be for point $i$ to choose point $k$ as its exemplar, taking into account the support from other points that point $k$ should be an exemplar\" [frey-2007]. Equation (2) from [frey-2007]:\n",
    " \n",
    " \\begin{equation*}\n",
    "   a(i, k) \\leftarrow \\min \\{ 0, r(k,k) + \\sum_{i' ~ s.t.~ i' \\notin \\{i,k\\}} \\max \\{ 0, r(i',k) \\} \\}\n",
    " \\end{equation*}\n",
    " \n",
    "This algorithm is not scalable to large datasets. Running the command below uses nearly 200GB of ram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "est_aff, proc_aff = bench_clust([AffinityPropagation()] ,[\"auto\"], data=mnist.data, labels=mnist.target, est_type='affinity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in proc_aff:\n",
    "    p.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agglomerative Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering) <a id='agglomerative'></a>\n",
    "\n",
    "A bottom up hierarchical clustering approach. This means each sample begins as its own singleton cluster and the two closest clusters are successively merged. There are several popular linkage criterion.\n",
    "\n",
    "+ Ward - This approach minimizes variance, like k-means. This is achieved by minimizing the squared differences within the clusters.\n",
    "+ Complete linkage - This approach minimizes the maximum distance between samples in pairs of clusters.\n",
    "+ Average linkage - This approach minimizes the average distance between all samples in pairs of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title    \t   time \thomog\tcomp\tv mes\trand\tmutu\n",
      "ward n=5 \t2238.60s\t0.486\t0.779\t0.599\t0.351\t0.486\n",
      "camp n=10\t2242.07s\t0.260\t0.334\t0.292\t0.130\t0.260\n",
      "ward n=3 \t2250.85s\t0.284\t0.718\t0.407\t0.188\t0.284\n",
      "ward n=15\t2264.27s\t0.718\t0.633\t0.673\t0.460\t0.633\n",
      "ward n=10\t2264.96s\t0.673\t0.691\t0.682\t0.527\t0.673\n",
      "avg n=10 \t2276.93s\t0.093\t0.690\t0.164\t0.029\t0.093\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "est_agg, proc_agg = bench_clust(\n",
    "          [ AgglomerativeClustering(n_clusters=3, linkage=\"ward\"),\n",
    "            AgglomerativeClustering(n_clusters=5, linkage=\"ward\"),\n",
    "            AgglomerativeClustering(n_clusters=10, linkage=\"ward\"),\n",
    "            AgglomerativeClustering(n_clusters=10, linkage=\"complete\"),\n",
    "            AgglomerativeClustering(n_clusters=10, linkage=\"average\"),\n",
    "            AgglomerativeClustering(n_clusters=15, linkage=\"ward\") ],\n",
    "          [ \"ward n=3\", \"ward n=5\", \"ward n=10\", \"camp n=10\", \"avg n=10\", \"ward n=15\" ], \n",
    "                    data=mnist.data, labels=mnist.target, est_type='agglom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in proc_agg:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Gaussian Mixture Modles (GMM)](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html) <a id='birch'></a>\n",
    "\n",
    "It is a memory-efficient, online-learning algorithm provided as an alternative to MiniBatchKMeans. It constructs a tree data structure with the cluster centroids being read off the leaf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Balanced Iterative Reducing and Clustering using Hierarchies (Birch)](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html) <a id='birch'></a>\n",
    "\n",
    "[Paper](https://rdcu.be/XdFp)  \n",
    "It is a memory-efficient, online-learning algorithm provided as an alternative to MiniBatchKMeans. It constructs a tree data structure with the cluster centroids being read off the leaf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibtex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K means article [1]\n",
    "\n",
    "@article{zhao-2018-k-means-a-revisit,\n",
    "title = \"k-means: A revisit\",\n",
    "journal = \"Neurocomputing\",\n",
    "volume = \"291\",\n",
    "pages = \"195 - 206\",\n",
    "year = \"2018\",\n",
    "issn = \"0925-2312\",\n",
    "doi = \"https://doi.org/10.1016/j.neucom.2018.02.072\",\n",
    "url = \"http://www.sciencedirect.com/science/article/pii/S092523121830239X\",\n",
    "author = \"Zhao, Wan-Lei and Deng, Cheng-Hao and Chong-Wah Ngo, Chong-Wah\",\n",
    "keywords = \"Clustering, -means, Incremental optimization\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN article [2] \n",
    "\n",
    "@inproceedings{ester-1996-a-density-based-algorithm-for-discovering-clusters-a-density-based-algorithm-for-discovering-clusters-in-large-spatial-databases-with-noise,\n",
    " author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\\\"{o}rg and Xu, Xiaowei},\n",
    " title = {A Density-based Algorithm for Discovering Clusters a Density-based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},\n",
    " booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},\n",
    " series = {KDD'96},\n",
    " year = {1996},\n",
    " location = {Portland, Oregon},\n",
    " pages = {226--231},\n",
    " numpages = {6},\n",
    " url = {http://dl.acm.org/citation.cfm?id=3001460.3001507},\n",
    " acmid = {3001507},\n",
    " publisher = {AAAI Press},\n",
    " keywords = {arbitrary shape of clusters, clustering algorithms, efficiency on large spatial databases, handling nlj4-275oise},\n",
    " } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V-Measure article [3]\n",
    "\n",
    "@inproceedings{rosenberg-2007-proceedings-of-the-2007-joint-conference-on-empirical-methods-in-natural-language-processing-and-computational-natural-language-learning,\n",
    "  title={V-measure: A conditional entropy-based external cluster evaluation measure},\n",
    "  author={Rosenberg, Andrew and Hirschberg, Julia},\n",
    "  booktitle={Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL)},\n",
    "  year={2007},\n",
    "  url = {http://aclweb.org/anthology/D/D07/D07-1043.pdf},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rand Index article [4]\n",
    "\n",
    "@Article{hubert-1985-comparing-partitions,\n",
    "author=\"Hubert, Lawrence\n",
    "and Arabie, Phipps\",\n",
    "title=\"Comparing partitions\",\n",
    "journal=\"Journal of Classification\",\n",
    "year=\"1985\",\n",
    "month=\"Dec\",\n",
    "day=\"01\",\n",
    "volume=\"2\",\n",
    "number=\"1\",\n",
    "pages=\"193--218\",\n",
    "abstract=\"The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between {\\textpm}1.\",\n",
    "issn=\"1432-1343\",\n",
    "doi=\"10.1007/BF01908075\",\n",
    "url=\"https://doi.org/10.1007/BF01908075\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusted Mutual Information article [5]\n",
    "\n",
    "@article{vinh-2010-information-theoretic-measures-for-clusterings-comparison-variants,-properties,-normalization-and-correction-for-chance,\n",
    " author = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},\n",
    " title = {Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance},\n",
    " journal = {J. Mach. Learn. Res.},\n",
    " issue_date = {3/1/2010},\n",
    " volume = {11},\n",
    " month = dec,\n",
    " year = {2010},\n",
    " issn = {1532-4435},\n",
    " pages = {2837--2854},\n",
    " numpages = {18},\n",
    " url = {http://dl.acm.org/citation.cfm?id=1756006.1953024},\n",
    " acmid = {1953024},\n",
    "\n",
    "publisher = {JMLR.org},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affinity Propagation article [6]\n",
    "\n",
    "@article {frey-2007-clustering-by-passing-messages-between-data-points,\n",
    "  author = {Frey, Brendan J. and Dueck, Delbert},\n",
    "  title = {Clustering by Passing Messages Between Data Points},\n",
    "  volume = {315},\n",
    "  number = {5814},\n",
    "  pages = {972--976},\n",
    "  year = {2007},\n",
    "  doi = {10.1126/science.1136800},\n",
    "  publisher = {American Association for the Advancement of Science},\n",
    "  abstract = {Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such {\\textquotedblleft}exemplars{\\textquotedblright} can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called {\\textquotedblleft}affinity propagation,{\\textquotedblright} which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time.},\n",
    "  issn = {0036-8075},\n",
    "  URL = {http://science.sciencemag.org/content/315/5814/972},\n",
    "  eprint = {http://science.sciencemag.org/content/315/5814/972.full.pdf},\n",
    "  journal = {Science}\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
