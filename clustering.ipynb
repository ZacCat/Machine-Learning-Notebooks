{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [4 General Functions (Review)](#general_functions)\n",
    "    * [4.1 Functions to Save and Open Variables](#open_save)\n",
    "* [5 Document Clustering](#document_clustering) \n",
    "    * [5.1 k-Means Clustering](#k_means)\n",
    "    * [5.2 Density-based Spatial Clustering of Applications with Noise (DBSCAN)](#dbscan)\n",
    "    * [5.3 Balanced Iterative Reducing and Clustering using Hierarchies (Birch)](#birch)\n",
    "    * [5.4 Affinity Propagation](#prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions <a id='general_functions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Save and Open Variables <a id='open_save'></a>\n",
    "\n",
    "Since it is not uncommon for a machine learning task to take a long time it is good practice to save variables that may be needed in the future. This can be achieved by using the [pickle](https://docs.python.org/3/library/pickle.html) module. This package allows a variable up to 4gb to be saved. This limitation is why the 'metrics' variables are saved as individual items instead of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save variables to file\n",
    "import pickle\n",
    "\n",
    "def save_var(variable_name):\n",
    "    \"\"\" Saves the variable with the provided variable name \n",
    "         in the global namespace to the ./vars folder \n",
    "         with the provided same name \"\"\"\n",
    "    \n",
    "    with open('./vars/' + variable_name,'wb') as my_file_obj:\n",
    "        pickle.dump(globals()[variable_name], my_file_obj, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_var_list(variable_name_list):\n",
    "    \"\"\" Saves each variable with the provided variable name \n",
    "         in the global namespace to the ./vars folder \n",
    "         with the provided same name \"\"\"\n",
    "    for name in variable_name_list:\n",
    "        with open('./vars/' + name,'wb') as my_file_obj:\n",
    "            pickle.dump(globals()[name], my_file_obj, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def open_var(file_name):\n",
    "    \"\"\" Returns the variable saved with the provided \n",
    "         file name located in the ./vars folder\"\"\"\n",
    "    \n",
    "    file_object = open('./vars/' + file_name,'rb')  \n",
    "\n",
    "    loaded_var = pickle.load(file_object)\n",
    "    \n",
    "    return loaded_var\n",
    "\n",
    "def open_var_list(file_name_list):\n",
    "    \"\"\" Loads a variable corresponding to each file name\n",
    "         in file_name_list to the global namespace. \"\"\"\n",
    "    \n",
    "    for file_name in file_name_list:\n",
    "        globals()[file_name] = open_var(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 ms, sys: 52 ms, total: 64 ms\n",
      "Wall time: 61.1 ms\n"
     ]
    }
   ],
   "source": [
    "%time mnist = open_var('mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load Datasets\n",
    "class Dataset_Part:\n",
    "    \"\"\" Represents a dataset with attributes\n",
    "         data and target \"\"\"\n",
    "    \n",
    "    data = None\n",
    "    target = None\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "\n",
    "open_var_list(['mnist_train', 'mnist_test', 'rcv1_train', 'rcv1_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_digit(dataset, index):\n",
    "    # Get a random document\n",
    "    digit_arr = dataset.data[index]\n",
    "    # Reshape it to the size of the image\n",
    "    digit_image = digit_arr.reshape(28,28)\n",
    "\n",
    "    # Some information\n",
    "    print(f'\\tIndex: {index}\\tLabel: {dataset.target[index]:.0f}')\n",
    "    # Show the image\n",
    "    plt.imshow(digit_image, cmap=matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Document Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) <a id='document_clustering'></a>\n",
    "\n",
    "Clustering is an unsupervised training method, meaning it is performed on data without labels. Because of this unsupervised learning is capable of finding relations that may not have been previously observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Cluster Evaluation](http://scikit-learn.org/stable/modules/clustering.html#homogeneity-completeness-and-v-measure) <a id='cluster_evaluation'></a>\n",
    "\n",
    "Unsupervised learning uses different evaluation metrics than supervised learning. This is because unsupervised learning makes assumptions with no prior knowladge (ie. no labels). Since the data does not conform to predetermined labels evaluation metrics such as precision and recall cannot be performed. Instead the following metrics can be used.\n",
    "\n",
    "+ __homogeneity score__ - A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class. A value of 1.0 represents perfectly homogenious labeling.  \n",
    "$$ h = 1 - \\frac{H(C \\mid K)}{H(C)} $$   \n",
    "$$H(C \\mid K) = - \\sum^{\\mid C \\mid}_{c=1} \\sum^{\\mid K \\mid}_{k=1} \\frac{n_{c,k}}{n} \\cdot \\log(\\frac{n_{c,k}}{n_k})$$  \n",
    "$$H(C) = - \\sum^{\\mid C \\mid}_{c=1} \\frac{n_{c}}{n} \\cdot \\log(\\frac{n_{c}}{n})$$\n",
    "+ __completeness score__ - A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster. A value of 1.0 represents perfectly complete labeling.\n",
    "$$ c = 1 - \\frac{H(K \\mid C)}{H(K)} $$\n",
    "+ [__Rand index__](https://doi.org/10.1007/BF01908075) - Measure of similarity between the predicted and true clusters. Rand Index considers all pairs of samples and counts the number of pairs that are assigned correctly to the same cluster, incorrectly to the same cluster, correctly to seperate clusters, and incorrectly to different clusters.\n",
    "  + __TODO: PARAPHRASE ASSIGNMENT__\n",
    "  + If C is the ground truth of class assignment and K the clustering, let us define:\n",
    "    + a as the number of pairs of elements that are in the same set in C and in the same set in K\n",
    "    + b as the number of pairs of elements that are in different sets in C and in different sets in K\n",
    "    + $C^{n_{samples}}_2$ as the total number of possible pairs in the dataset (without ordering)\n",
    "$$ RI = \\frac{a + b}{C^{n_{samples}}_2}$$\n",
    "+ __adjusted Rand score__ - The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. \n",
    "$$ ARI = \\frac{RI - E(RI)}{\\max(RI) - E(RI)} $$\n",
    "  + Note - E(RI) means expected RI, or the RI given random labelings. \n",
    "+ __mutual information score__ - The measure of similarity between the predicted and true labels, ignoring permutation. Given two sets of clusters $V$ and $U$. Suppose $U$ has size $i$, denoted as $\\mid U \\mid = i$, and similarly for $\\mid V \\mid = j$.\n",
    "$$ MI(U,V) = \\sum^{\\mid U \\mid}_{i=1} \\sum^{\\mid V \\mid}_{j=1} \\frac{\\mid U_i \\cap V_j \\mid}{N} \\cdot log ( \\frac{N \\mid U_i \\cap V_j \\mid}{ \\mid U_i \\mid \\mid V_j \\mid}) $$\n",
    "+ __adjusted mutual info score__ - The mutual information score adjusted to account for the fact tha mutual information score is typically greater when there are more clusters. The adjusted mutual information score is 1 when the two sets of clusters are the same. Random clustering have an expected adjusted mutual information score near 0.\n",
    "$$ AMI(U,V) = \\frac{MI(U,V) - E(MI(U, V))} {\\max(H(U), H(V)) - E(MI(U, V))} $$\n",
    "+ __V-measure__ - The harmonic mean between homogeneity and completeness.\n",
    "$$ v = 2 \\cdot \\frac{ \\text{homogeneity} \\cdot \\text{completeness}}{\\text{homogeneity} + \\text{completeness}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "def bench_clust(estimator_lst, name_lst, data, labels):\n",
    "\n",
    "    print('%-9s\\t%-6s\\t%-4s\\t%-4s\\t%-4s\\t%-4s\\t%-4s' \n",
    "          % ( 'title', 'time', 'homog', 'comp', 'v mes',\n",
    "              'rand', 'mutu'))\n",
    "    est_lst = []\n",
    "    for estimator, name in zip(estimator_lst, name_lst):\n",
    "        t0 = time()\n",
    "        estimator.fit(data)\n",
    "        print('%-9s\\t%.2fs\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "              % (name, (time() - t0),\n",
    "                 metrics.homogeneity_score(labels, estimator.labels_),\n",
    "                 metrics.completeness_score(labels, estimator.labels_),\n",
    "                 metrics.v_measure_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_mutual_info_score(labels,  estimator.labels_)))\n",
    "\n",
    "        est_lst += [estimator]\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [k-Means Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) <a id='k_means'></a>\n",
    "\n",
    "This algorithm is implemented in the sklearn.cluster.KMeans scikit-learn module. K-means clustering attempts to seperate data into a predetermined, k, number of clusters. The aim is to create clusters with equal variance, thus minimizing inertia, also known as the within-cluster sum of squares. Inertia is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    " \\sum_{i=0}^n \\min_{\\mu_j \\in C} (\\mid \\mid x_j - \\mu_i \\mid \\mid^2)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "[comment]: <> (need to reword, too close to source)\n",
    "To find clusters k-Means has a three step process explained by [Zhao et al.](https://doi.org/10.1016/j.neucom.2018.02.072) [1] as:\n",
    "\n",
    "1. Initialize k centroids, one for each cluster. The most basic way to do this is by picking k random samples.\n",
    "+ Assign each sample to the closest centroid.\n",
    "+ Recompute centroids with assignments from previous step.\n",
    "+ Repeat step 2 and step 3 until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "def bench_k_means(estimator_lst, name_lst, data, labels):\n",
    "    print('%-9s\\t%-6s\\t%-12s\\t%-4s\\t%-4s\\t%-4s\\t%-4s\\t%-4s\\t%-4s' \n",
    "      % ( 'title', 'time', 'inertia', 'homog', 'comp', 'v mes', 'rand', 'mutu', 'silh'))\n",
    "    for estimator, name in zip(estimator_lst, name_lst):\n",
    "        t0 = time()\n",
    "        estimator.fit(data)\n",
    "        print('%-9s\\t%.2fs\\t%i\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "              % (name, (time() - t0), estimator.inertia_,\n",
    "                 metrics.homogeneity_score(labels, estimator.labels_),\n",
    "                 metrics.completeness_score(labels, estimator.labels_),\n",
    "                 metrics.v_measure_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_rand_score(labels, estimator.labels_),\n",
    "                 metrics.adjusted_mutual_info_score(labels,  estimator.labels_),\n",
    "                 metrics.silhouette_score(data, estimator.labels_,\n",
    "                                          metric='euclidean',\n",
    "                                          sample_size=1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title    \ttime  \tinertia     \thomog\tcomp\tv mes\trand\tmutu\tsilh\n",
      "k-means++ k=3\t17.01s\t213604855174\t0.211\t0.443\t0.286\t0.172\t0.211\t0.058\n",
      "k-means++ k=5\t19.18s\t197606837877\t0.390\t0.578\t0.465\t0.330\t0.389\t0.067\n",
      "k-means++ k=10\t28.63s\t178432235366\t0.496\t0.503\t0.500\t0.365\t0.496\t0.062\n",
      "random k=10\t20.22s\t178432527554\t0.496\t0.503\t0.500\t0.366\t0.496\t0.054\n",
      "k-means++ k=15\t24.53s\t167394422335\t0.577\t0.494\t0.532\t0.380\t0.493\t0.059\n",
      "random k=15\t32.07s\t167392804879\t0.579\t0.495\t0.534\t0.382\t0.495\t0.063\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "bench_k_means([KMeans(init='k-means++', n_clusters=3, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='k-means++', n_clusters=5, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='k-means++', n_clusters=10, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='random', n_clusters=10, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='k-means++', n_clusters=15, n_init=10, n_jobs=-1),\n",
    "               KMeans(init='random', n_clusters=15, n_init=10, n_jobs=-1) ],\n",
    "              [\"k-means++ k=3\", \"k-means++ k=5\", \"k-means++ k=10\", \"random k=10\", \"k-means++ k=15\", \"random k=15\"],\n",
    "              data=mnist.data, labels=mnist.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Density-based Spatial Clustering of Applications with Noise (DBSCAN)](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) <a id='dbscan'></a>\n",
    "\n",
    "The DBSCAN algorithm clusters samples into areas of high density with surrounding low dennsity areas. Because of this Clusters can be any shape and the number of clusters is not predeturmined. Clusters are formed by finding region that satisfy a minimum density, number of documents per area. The shape of the cluster is determined by the distance metric used. Any distance function can be used and the distance function will determine the shape of the clusters [2].\n",
    "\n",
    "To form a cluster DBSCAN searches for areas with a minimum number of points within a specified distance, $\\varepsilon$, from a central point, this area is called an $\\varepsilon$-neighborhood. Each point in a $\\varepsilon$-neighborhood will expand outward, and if this neighborhood meets the minimum number of points required the cluster is updated to include this $\\varepsilon$-neighborhood. Points that are not within $\\varepsilon$ of the center, but it is included in the cluster it is said to be density-reachable. [Good visualization here](https://cse.buffalo.edu/~jing/cse601/fa12/materials/clustering_density.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2422.3517784359951"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "ones = np.where(mnist.target == 2.)[0]\n",
    "result = []\n",
    "tot_result = 0.\n",
    "count = 0\n",
    "# result = np.linalg.norm(mnist.data[ones], 'fro')\n",
    "for k in ones:\n",
    "    for i in ones[random.sample(range(len(ones)), 100)]:\n",
    "        if k != i:\n",
    "            res = np.linalg.norm(mnist.data[i] - mnist.data[k])\n",
    "            result += [res]\n",
    "            tot_result += res\n",
    "            count += 1\n",
    "\n",
    "avg_result = tot_result / count\n",
    "avg_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "estimators = bench_clust([DBSCAN(n_jobs=-1),\n",
    "            DBSCAN(eps=1000, min_samples=10, n_jobs=-1),\n",
    "            DBSCAN(eps=100, min_samples=10, n_jobs=-1),\n",
    "            DBSCAN(eps=10, min_samples=100, n_jobs=-1),\n",
    "            DBSCAN(eps=10, min_samples=1000, n_jobs=-1)],\n",
    "            [\"auto\", \"1000, 10\", \"100,10\", \"10,100\", \"10,1000\"], data=mnist.data, labels=mnist.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Affinity Propagation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation) <a id='prop'></a>\n",
    "\n",
    "Affinity propagation uses exemplars instead of centroids. This means that instead of finding a centroid affinity propagation works by finding samples that are most representative of the other samples. In addition the number of clusters is not predetermined. The number of clusters is determined by the data. \n",
    "\n",
    "[comment]: <> (need to reword, too close to source)\n",
    "There are three main formulas used in affinity propagation.\n",
    "\n",
    "1. The similarity of two samples is denoted $s(i,k)$.\n",
    "2. The responsibility of a sample, $k$, to be an exemplar of sample, $i$. \n",
    "  \n",
    " \\begin{equation*} \n",
    "   r(i, k) \\leftarrow s(i,k) - \\max [a(i, k') + s(i, k') \\forall k' \\neq k] \n",
    " \\end{equation*}\n",
    "\n",
    "3. The accumulated evidence that sample $i$ should choose sample $k$ to be its exemplar.\n",
    " \n",
    " \\begin{equation*}\n",
    "   a(i, k) \\leftarrow \\min [0, r(k,k) + \\sum_{i' ~ s.t.~ i' \\notin \\{i,k\\}} r(i',k)]\n",
    " \\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title    \ttime  \tinertia     \thomog\tcomp\tv mes\trand\tmutu\tsilh\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "bench_clust([AffinityPropagation()] ,[\"auto\"], data=mnist.data, labels=mnist.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Gaussian Mixture Modles (GMM)](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html) <a id='birch'></a>\n",
    "\n",
    "[Paper](https://rdcu.be/XdFp)  \n",
    "It is a memory-efficient, online-learning algorithm provided as an alternative to MiniBatchKMeans. It constructs a tree data structure with the cluster centroids being read off the leaf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Balanced Iterative Reducing and Clustering using Hierarchies (Birch)](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html) <a id='birch'></a>\n",
    "\n",
    "[Paper](https://rdcu.be/XdFp)  \n",
    "It is a memory-efficient, online-learning algorithm provided as an alternative to MiniBatchKMeans. It constructs a tree data structure with the cluster centroids being read off the leaf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibtex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K means article [1]\n",
    "@article{ZHAO2018195,\n",
    "title = \"k-means: A revisit\",\n",
    "journal = \"Neurocomputing\",\n",
    "volume = \"291\",\n",
    "pages = \"195 - 206\",\n",
    "year = \"2018\",\n",
    "issn = \"0925-2312\",\n",
    "doi = \"https://doi.org/10.1016/j.neucom.2018.02.072\",\n",
    "url = \"http://www.sciencedirect.com/science/article/pii/S092523121830239X\",\n",
    "author = \"Wan-Lei Zhao and Cheng-Hao Deng and Chong-Wah Ngo\",\n",
    "keywords = \"Clustering, -means, Incremental optimization\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN article [2] \n",
    "@inproceedings{Ester:1996:DAD:3001460.3001507,\n",
    " author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\\\"{o}rg and Xu, Xiaowei},\n",
    " title = {A Density-based Algorithm for Discovering Clusters a Density-based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},\n",
    " booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},\n",
    " series = {KDD'96},\n",
    " year = {1996},\n",
    " location = {Portland, Oregon},\n",
    " pages = {226--231},\n",
    " numpages = {6},\n",
    " url = {http://dl.acm.org/citation.cfm?id=3001460.3001507},\n",
    " acmid = {3001507},\n",
    " publisher = {AAAI Press},\n",
    " keywords = {arbitrary shape of clusters, clustering algorithms, efficiency on large spatial databases, handling nlj4-275oise},\n",
    " } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V-Measuer article [3]\n",
    "\n",
    "@inproceedings{rosenberg2007v,\n",
    "  title={V-measure: A conditional entropy-based external cluster evaluation measure},\n",
    "  author={Rosenberg, Andrew and Hirschberg, Julia},\n",
    "  booktitle={Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL)},\n",
    "  year={2007},\n",
    "  url = {http://aclweb.org/anthology/D/D07/D07-1043.pdf},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rand Index article [4]\n",
    "\n",
    "@Article{Hubert1985,\n",
    "author=\"Hubert, Lawrence\n",
    "and Arabie, Phipps\",\n",
    "title=\"Comparing partitions\",\n",
    "journal=\"Journal of Classification\",\n",
    "year=\"1985\",\n",
    "month=\"Dec\",\n",
    "day=\"01\",\n",
    "volume=\"2\",\n",
    "number=\"1\",\n",
    "pages=\"193--218\",\n",
    "abstract=\"The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between {\\textpm}1.\",\n",
    "issn=\"1432-1343\",\n",
    "doi=\"10.1007/BF01908075\",\n",
    "url=\"https://doi.org/10.1007/BF01908075\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
