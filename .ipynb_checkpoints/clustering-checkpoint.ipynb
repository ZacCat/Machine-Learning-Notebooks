{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "* [4 General Functions (Review)](#general_functions)\n",
    "    * [4.1 Functions to Save and Open Variables](#open_save)\n",
    "* [5 Document Clustering](#document_clustering) \n",
    "    * [5.1 k-Means Clustering](#k_means)\n",
    "    * [5.2 Density-based Spatial Clustering of Applications with Noise (DBSCAN)](#dbscan)\n",
    "    * [5.3 Balanced Iterative Reducing and Clustering using Hierarchies (Birch)](#birch)\n",
    "    * [5.4 Affinity Propagation](#prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Functions <a id='general_functions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to Save and Open Variables <a id='open_save'></a>\n",
    "\n",
    "Since it is not uncommon for a machine learning task to take a long time it is good practice to save variables that may be needed in the future. This can be achieved by using the [pickle](https://docs.python.org/3/library/pickle.html) module. This package allows a variable up to 4gb to be saved. This limitation is why the 'metrics' variables are saved as individual items instead of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save variables to file\n",
    "import pickle\n",
    "\n",
    "def save_var(variable_name):\n",
    "    \"\"\" Saves the variable with the provided variable name \n",
    "         in the global namespace to the ./vars folder \n",
    "         with the provided same name \"\"\"\n",
    "    \n",
    "    with open('./vars/' + variable_name,'wb') as my_file_obj:\n",
    "        pickle.dump(globals()[variable_name], my_file_obj, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_var_list(variable_name_list):\n",
    "    \"\"\" Saves each variable with the provided variable name \n",
    "         in the global namespace to the ./vars folder \n",
    "         with the provided same name \"\"\"\n",
    "    for name in variable_name_list:\n",
    "        with open('./vars/' + name,'wb') as my_file_obj:\n",
    "            pickle.dump(globals()[name], my_file_obj, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def open_var(file_name):\n",
    "    \"\"\" Returns the variable saved with the provided \n",
    "         file name located in the ./vars folder\"\"\"\n",
    "    \n",
    "    file_object = open('./vars/' + file_name,'rb')  \n",
    "\n",
    "    loaded_var = pickle.load(file_object)\n",
    "    \n",
    "    return loaded_var\n",
    "\n",
    "def open_var_list(file_name_list):\n",
    "    \"\"\" Loads a variable corresponding to each file name\n",
    "         in file_name_list to the global namespace. \"\"\"\n",
    "    \n",
    "    for file_name in file_name_list:\n",
    "        globals()[file_name] = open_var(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 175 ms, sys: 1.02 s, total: 1.2 s\n",
      "Wall time: 4.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Load Datasets\n",
    "class Dataset_Part:\n",
    "    \"\"\" Represents a dataset with attributes\n",
    "         data and target \"\"\"\n",
    "    \n",
    "    data = None\n",
    "    target = None\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.target = y\n",
    "\n",
    "open_var_list(['mnist_train', 'mnist_test', 'rcv1_train', 'rcv1_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "open_var_list(['scores_NB_rcv1', 'scores_Mult_NB', 'scores_Gauss_NB', 'scores_Bern_NB','scores_SVM_mnist', \n",
    "               'scores_kNN_mnist', 'scores_DT_rcv1', 'scores_DT_mnist'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "open_var_list(['clf_SVM_mnist', 'pred_SVM_mnist', 'scores_SVM_mnist', 'confidence_SVM_mnist', 'metrics_SVM_mnist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Document Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) <a id='document_clustering'></a>\n",
    "\n",
    "Clustering is an unsupervised training method, meaning it is performed on data without labels. Because of this unsupervised learning is capable of finding relations that may not have been previously observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [k-Means Clustering](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) <a id='k_means'></a>\n",
    "\n",
    "This algorithm is implemented in the sklearn.cluster.KMeans scikit-learn module. K-means clustering attempts to seperate data into a predetermined, k, number of clusters. The aim is to create clusters with equal variance, thus minimizing inertia, also known as the within-cluster sum of squares. Inertia is defined as:\n",
    "\n",
    "\\begin{equation*}\n",
    " \\sum_{i=0}^n \\min_{\\mu_j \\in C} (\\mid \\mid x_j - \\mu_i \\mid \\mid^2)\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "[comment]: <> (need to reword, too close to source)\n",
    "To find clusters k-Means has a three step process explained by [Zhao et al.](https://doi.org/10.1016/j.neucom.2018.02.072) [1] as:\n",
    "\n",
    "1. Initialize k centroids, one for each cluster. The most basic way to do this is by picking k random samples.\n",
    "+ Assign each sample to the closest centroid.\n",
    "+ Recompute centroids with assignments from previous step.\n",
    "+ Repeat step 2 and step 3 until convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Density-based Spatial Clustering of Applications with Noise (DBSCAN)](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN) <a id='dbscan'></a>\n",
    "\n",
    "The DBSCAN algorithm clusters samples into areas of high density with surrounding low dennsity areas. Because of this Clusters can be any shape and the number of clusters is not predeturmined. Clusters are formed by finding region that satisfy a minimum density, number of documents per area. The shape of the cluster is determined by the distance metric used. Any distance function can be used and the distance function will determine the shape of the clusters [2].\n",
    "\n",
    "To form a cluster DBSCAN searches for areas with a minimum number of points within a specified distance, $\\varepsilon$, from a central point, this area is called an $\\varepsilon$-neighborhood. Each point in a $\\varepsilon$-neighborhood will expand outward, and if this neighborhood meets the minimum number of points required the cluster is updated to include this $\\varepsilon$-neighborhood. Points that are not within $\\varepsilon$ of the center, but it is included in the cluster it is said to be density-reachable. [Good visualization here](https://cse.buffalo.edu/~jing/cse601/fa12/materials/clustering_density.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Balanced Iterative Reducing and Clustering using Hierarchies (Birch)](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html) <a id='birch'></a>\n",
    "\n",
    "[Paper](https://rdcu.be/XdFp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Affinity Propagation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html#sklearn.cluster.AffinityPropagation) <a id='prop'></a>\n",
    "\n",
    "Affinity propagation uses exemplars instead of centroids. This means that instead of finding a centroid affinity propagation works by finding samples that are most representative of the other samples. In addition the number of clusters is not predetermined. The number of clusters is determined by the data. \n",
    "\n",
    "[comment]: <> (need to reword, too close to source)\n",
    "There are three main formulas used in affinity propagation.\n",
    "\n",
    "1. The similarity of two samples is denoted $s(i,k)$.\n",
    "2. The responsibility of a sample, $k$, to be an exemplar of sample, $i$. \n",
    "  \n",
    " \\begin{equation*} \n",
    "   r(i, k) \\leftarrow s(i,k) - \\max [a(i, k') + s(i, k') \\forall k' \\neq k] \n",
    " \\end{equation*}\n",
    "\n",
    "3. The accumulated evidence that sample $i$ should choose sample $k$ to be its exemplar.\n",
    " \n",
    " \\begin{equation*}\n",
    "   a(i, k) \\leftarrow \\min [0, r(k,k) + \\sum_{i' ~ s.t.~ i' \\notin \\{i,k\\}} r(i',k)]\n",
    " \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibtex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K means article [1]\n",
    "@article{ZHAO2018195,\n",
    "title = \"k-means: A revisit\",\n",
    "journal = \"Neurocomputing\",\n",
    "volume = \"291\",\n",
    "pages = \"195 - 206\",\n",
    "year = \"2018\",\n",
    "issn = \"0925-2312\",\n",
    "doi = \"https://doi.org/10.1016/j.neucom.2018.02.072\",\n",
    "url = \"http://www.sciencedirect.com/science/article/pii/S092523121830239X\",\n",
    "author = \"Wan-Lei Zhao and Cheng-Hao Deng and Chong-Wah Ngo\",\n",
    "keywords = \"Clustering, -means, Incremental optimization\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN article [2] \n",
    "@inproceedings{Ester:1996:DAD:3001460.3001507,\n",
    " author = {Ester, Martin and Kriegel, Hans-Peter and Sander, J\\\"{o}rg and Xu, Xiaowei},\n",
    " title = {A Density-based Algorithm for Discovering Clusters a Density-based Algorithm for Discovering Clusters in Large Spatial Databases with Noise},\n",
    " booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},\n",
    " series = {KDD'96},\n",
    " year = {1996},\n",
    " location = {Portland, Oregon},\n",
    " pages = {226--231},\n",
    " numpages = {6},\n",
    " url = {http://dl.acm.org/citation.cfm?id=3001460.3001507},\n",
    " acmid = {3001507},\n",
    " publisher = {AAAI Press},\n",
    " keywords = {arbitrary shape of clusters, clustering algorithms, efficiency on large spatial databases, handling nlj4-275oise}, } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
